# Content Backlog

## How This Works

- **Unchecked `[ ]`** = Suggested topic, pending human approval
- **Checked `[x]`** = Approved for development (Cursor Agent may work on these)
- **Completed items** = Move to the Completed section at the bottom

Cursor Agent: Only work on checked items. Add new suggestions as unchecked items.

---

## Approved for Development

Check a box to approve a topic. Cursor Agent will work through these.

- [x] **tokens**: What are tokens?
  - Scope: Tokenization, why models use tokens not words, vocabulary, text-to-numbers
  - From: what-is-llm → leads to: context-window

- [x] **why-large**: Why does scale matter?
  - Scope: Emergent capabilities, scaling laws, size-capability relationship
  - From: what-is-llm → leads to: training, parameters

- [ ] **context-window**: The context window
  - Scope: What context is, why limited, conversation "memory"
  - From: tokens → leads to: attention

---

## Suggestions (Pending Approval)

Cursor Agent adds suggestions here. Check to approve.

- [ ] **training**: How are LLMs trained?
  - Scope: Pre-training, prediction objective, compute requirements
  - Rationale: Natural next question after understanding what LLMs do

- [ ] **hallucinations**: Why do LLMs make things up?
  - Scope: What hallucination is, why it happens, mitigation
  - Rationale: Common user concern, mentioned in intro

- [ ] **understanding-debate**: Do LLMs really understand?
  - Scope: Philosophical debate, Chinese Room, behavioral vs phenomenal
  - Rationale: Expands on "Is prediction understanding?" aside

- [ ] **temperature**: What is "temperature" in AI?
  - Scope: Temperature parameter, sampling, creativity vs accuracy
  - Rationale: Expands on "Why add randomness?" aside

- [ ] **attention**: How does attention work?
  - Scope: Intuition for attention mechanism, self-attention basics
  - Rationale: Core mechanism enabling context understanding

- [ ] **prompt-engineering**: Communicating effectively with LLMs
  - Scope: Why prompts matter, techniques, pitfalls
  - Rationale: Practical application of LLM understanding

---

## Completed

- [x] **what-is-llm**: What is an LLM? *(2024-12-08)*

---

## Not In Scope (External Links)

Topics better covered elsewhere. Link to these rather than creating nodes.

<!-- Add rejected topics here with links to good external resources -->
