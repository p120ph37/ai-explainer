# Content Backlog

## How This Works

- **Unchecked `[ ]`** = Suggested topic, pending human approval
- **Checked `[x]`** = Approved for development (Cursor Agent may work on these)
- **Completed items** = Move to the Completed section at the bottom

Cursor Agent: Only work on checked items. Add new suggestions as unchecked items.

---

## Approved for Development

Check a box to approve a topic. Cursor Agent will work through these.

*(No items currently approved)*

---

## Suggestions (Pending Approval)

Cursor Agent adds suggestions here. Check to approve.

- [ ] **mcp**: What is Model Context Protocol?
  - Scope: MCP specification, standardized tool integration, ecosystem
  - Rationale: Emerging standard for AI tool use; split from tools topic

- [ ] **rag**: What is Retrieval-Augmented Generation?
  - Scope: RAG pattern, combining retrieval with generation, grounding responses
  - Rationale: Key technique for reducing hallucinations; split from vector-databases topic

- [ ] **alignment**: Why is AI alignment hard?
  - Scope: The alignment problem, value specification, instrumental convergence, current approaches
  - Rationale: Central concern in AI safety; expands on reward topic

- [ ] **multimodal**: How do AI models see and hear?
  - Scope: Vision models, audio, video, how LLMs expand beyond text
  - Rationale: Increasingly relevant as GPT-4V, Gemini, Claude vision become standard

- [ ] **open**: Open vs closed AI models
  - Scope: Open weights (Llama, Mistral) vs proprietary (GPT, Claude), trade-offs, implications
  - Rationale: Important ecosystem context; helps readers understand model landscape

- [ ] **bias**: Where does AI bias come from?
  - Scope: Training data bias, amplification, measurement, mitigation approaches
  - Rationale: Common concern; helps readers critically evaluate model outputs

---

## Completed

- [x] **what-is-llm**: What is an LLM? *(2024-12-08)*
- [x] **tokens**: What are tokens? *(2024-12-08)*
- [x] **why-large**: Why does scale matter? *(2024-12-08)*
- [x] **context-window**: The context window *(2024-12-08)*
- [x] **neural-network**: What is a neural network? *(2024-12-08)*
- [x] **parameters**: What are parameters? *(2024-12-08)*
- [x] **embeddings**: How do tokens become numbers? *(2024-12-08)*
- [x] **attention**: How does attention work? *(2024-12-08)*
- [x] **transformer**: What is a Transformer? *(2024-12-08)*
- [x] **labels**: What is labeled data? *(2024-12-08)*
- [x] **training**: How are LLMs trained? *(2024-12-08)*
- [x] **reward**: How do AI systems learn what's "good"? *(2024-12-08)*
- [x] **tuning**: How are LLMs customized? *(2024-12-08)*
- [x] **inference**: How does text generation actually happen? *(2024-12-08)*
- [x] **temperature**: What is temperature in AI? *(2024-12-08)*
- [x] **emergence**: What is emergent behavior? *(2024-12-08)*
- [x] **hallucinations**: Why do LLMs make things up? *(2024-12-08)*
- [x] **understanding**: Do LLMs really understand? *(2024-12-08)*
- [x] **prompt-engineering**: Communicating effectively with LLMs *(2024-12-08)*
- [x] **tools**: How do LLMs use tools? *(2024-12-08)*
- [x] **vector-databases**: What are vector databases? *(2024-12-08)*
- [x] **hardware**: What hardware runs AI? *(2024-12-08)*

---

## Not In Scope (External Links)

Topics better covered elsewhere. Link to these rather than creating nodes.

<!-- Add rejected topics here with links to good external resources -->
