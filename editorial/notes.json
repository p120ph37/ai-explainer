{
  "version": 1,
  "lastUpdated": "2025-12-15T21:00:00.000Z",
  "notes": [
    {
      "id": "mj0nr47a-9aeijp",
      "pageId": "tokens",
      "variantId": null,
      "anchor": {
        "type": "page"
      },
      "anchorValid": true,
      "content": "User reaction to consider/anticipate/address: Wait, so \"tokens\" in relation to LLMs means something other than just \"credits?\"  This seems to be a common conflation: that \"tokens\" are meant in the sense of the brass slugs dropped into slots at a video-arcade, but in reality, although some providers may conflate that sense into their use of the term, the primary idea of \"tokens\" in relation to LLMs is more like game-pieces -- checkers, chess-pieces, puzzle-pieces -- small abstract objects that represent some more-complex idea and have certain semantic relationships to each-other.  Of course, arcade-tokens are one tiny subset of this in that they represent units-of-value, but to hear \"tokens\" in relation to LLMs and think that it only means \"pricing units\" is to miss the technical side of the homonym entirely.  All of this isn't helped by the fact that providers often charge for usage by counting tokens...",
      "status": "open",
      "priority": "medium",
      "tags": [],
      "createdAt": "2025-12-10T23:46:10.822Z",
      "updatedAt": "2025-12-10T23:46:10.822Z",
      "author": "editor",
      "responses": []
    },
    {
      "id": "mj0p74sf-89hdi8",
      "pageId": "tokens",
      "variantId": "research-voice",
      "anchor": {
        "type": "text-selection",
        "selectedText": "Interactive exploration",
        "startOffset": 454,
        "endOffset": 477,
        "containerPath": "div#app > div.ssr-shell > main.app-main > div.content-width > article.content-node.variant-content > div.content-node__body > p > strong",
        "contextBefore": " tokenization: https://github.com/openai/tiktoken\n",
        "contextAfter": ": The Tiktokenizer web tool lets you see exactly h"
      },
      "anchorValid": true,
      "content": "We now have our own embedable tokenizer widget, and the external playground I prefer to recommend is: https://gpt-tokenizer.dev/",
      "status": "open",
      "priority": "medium",
      "tags": [],
      "createdAt": "2025-12-11T00:26:37.695Z",
      "updatedAt": "2025-12-11T00:26:37.695Z",
      "author": "editor",
      "responses": []
    },
    {
      "id": "mj0pa5dc-90o60s",
      "pageId": "tokens",
      "variantId": "research-voice",
      "anchor": {
        "type": "text-selection",
        "selectedText": "Typical vocabulary sizes are 50,000-100,000 tokens",
        "startOffset": 1215,
        "endOffset": 1265,
        "containerPath": "div#app > div.ssr-shell > main.app-main > div.content-width > article.content-node.variant-content > div.content-node__body > h3",
        "contextBefore": "rdPiece (similar but scores differently)\n\n\nClaim: ",
        "contextAfter": "\nDocumented vocabulary sizes:\n\nGPT-2: 50,257 token"
      },
      "anchorValid": true,
      "content": "Based on the \"o200k\" data from the tiktoken implementation, which is used for GPT-4o and GPT-5, we know that those use a 200k vocabulary.",
      "status": "open",
      "priority": "medium",
      "tags": [],
      "createdAt": "2025-12-11T00:28:58.416Z",
      "updatedAt": "2025-12-11T00:28:58.416Z",
      "author": "editor",
      "responses": []
    },
    {
      "id": "mj0pdmid-uu14vy",
      "pageId": "tokens",
      "variantId": "research-voice",
      "anchor": {
        "type": "text-selection",
        "selectedText": "Strawberry example",
        "startOffset": 2161,
        "endOffset": 2179,
        "containerPath": "div#app > div.ssr-shell > main.app-main > div.content-width > article.content-node.variant-content > div.content-node__body > p > strong",
        "contextBefore": "ices significantly affect what models can recall.\n",
        "contextAfter": ": The \"how many r's in strawberry\" failure became "
      },
      "anchorValid": true,
      "content": "conversely, the o200k model used by 4o and 5 tokenizes \"strawberry\" as a single token.  I wonder if there is any follow-up research paper or article that explores this and its implications?",
      "status": "open",
      "priority": "medium",
      "tags": [],
      "createdAt": "2025-12-11T00:31:40.598Z",
      "updatedAt": "2025-12-11T00:31:40.598Z",
      "author": "editor",
      "responses": []
    },
    {
      "id": "mj0pg9ae-awsmvf",
      "pageId": "tokens",
      "variantId": "research-voice",
      "anchor": {
        "type": "text-selection",
        "selectedText": "Tiktokenizer web tool",
        "startOffset": 5573,
        "endOffset": 5594,
        "containerPath": "div#app > div.ssr-shell > main.app-main > div.content-width > article.content-node.variant-content > div.content-node__body > ol > li",
        "contextBefore": "video (most accessible comprehensive explanation)\n",
        "contextAfter": " (hands-on exploration)\nOriginal BPE paper (for hi"
      },
      "anchorValid": true,
      "content": "Prefer https://gpt-tokenizer.dev/",
      "status": "open",
      "priority": "medium",
      "tags": [],
      "createdAt": "2025-12-11T00:33:43.430Z",
      "updatedAt": "2025-12-11T00:33:43.430Z",
      "author": "editor",
      "responses": []
    }
  ]
}
