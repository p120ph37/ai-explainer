---
id: training
title: "How are LLMs trained?"
summary: LLMs learn by predicting text billions of times, adjusting their parameters to make better predictions. This simple process, at massive scale, produces remarkable capabilities.
category: Foundations
order: 10
prerequisites:
  - neural-network
  - parameters
children:
  - tuning
  - reward
related:
  - labels
  - hardware
keywords:
  - training
  - pre-training
  - backpropagation
  - gradient descent
  - loss
---

import { Expandable, Metaphor, Question, Recognition, TryThis, Sources, Citation } from '../app/components/content/index.ts'
import { FlowDiagram } from '../app/components/diagrams/index.ts'

**How do you teach a neural network language?**

You show it text. An enormous amount of text. And you ask it, over and over: what word comes next?

The model starts with random parameters (billions of numbers that mean nothing). Its predictions are gibberish. But each wrong prediction provides feedback. The parameters adjust slightly. Over billions of examples, the adjustments accumulate into something that understands language.

This is **pre-training**: the massive first phase where an LLM learns the patterns of language itself.

<Recognition title="How does it know about THAT?">

You've probably been surprised by the *breadth* of topics an LLM can discuss. "How does it know about 15th-century Venetian glassmaking AND Python debugging AND that obscure movie?" The answer: it trained on trillions of tokens from everywhere. You may also have experienced the knowledge cutoff: asking about recent events and getting "I don't have information after [date]." That's the training data boundary made tangible.

</Recognition>

**The training loop**

Training follows a simple cycle:

<FlowDiagram
  title="The Training Loop"
  steps={[
    { id: '1', label: 'Sample Batch', sublabel: 'Get text', icon: 'ðŸ“¦' },
    { id: '2', label: 'Forward Pass', sublabel: 'Make predictions', icon: 'âž¡ï¸' },
    { id: '3', label: 'Compute Loss', sublabel: 'How wrong?', icon: 'ðŸ“‰' },
    { id: '4', label: 'Backward Pass', sublabel: 'Find gradients', icon: 'â¬…ï¸' },
    { id: '5', label: 'Update', sublabel: 'Adjust params', icon: 'ðŸ”„' },
  ]}
  direction="horizontal"
  ariaLabel="Flow diagram showing the training loop: sample, forward, loss, backward, update"
/>

Each iteration improves the model imperceptibly. But imperceptible improvements compound. After processing trillions of tokens, the model has learned patterns no human could articulate.

<Metaphor title="The gradient river">

Imagine a vast landscape where altitude represents error. The model starts somewhere random, usually on a high plateau where predictions are wrong.

Training is descending this landscape. At each step, you feel which direction slopes downward. That's the gradient. Take a small step in that direction. Repeat.

The landscape has billions of dimensions, one for each parameter. You're not walking down a hillside; you're navigating hyperspace, feeling for descent in directions that have no names.

Remarkably, the descent finds meaningful valleys. The landscape was shaped by human language, so its low points correspond to configurations that understand human language. Walk downhill long enough and you arrive somewhere that predicts text well.

</Metaphor>

**What data do they train on?**

Scale requires enormous datasets:

- **Web crawls**: Common Crawl provides billions of web pages
- **Books**: Digital libraries and published text
- **Code**: GitHub repositories and documentation
- **Conversations**: Reddit, forums, dialogue datasets
- **Academic papers**: Scientific literature across domains
- **Wikipedia**: Encyclopedic knowledge

A typical large model trains on trillions of tokens from diverse sources. The diversity matters: it's why LLMs can discuss Shakespeare and Python, cooking and quantum physics. The model sees the full breadth of what humans write about.

<Question title="Won't training on the web teach the model wrong things?">

Yes, and it does. The model learns patterns from all text, accurate or not. Misinformation patterns are in there. Post-training (fine-tuning, RLHF) tries to shape behavior toward accuracy, but the base model has absorbed everything.

This is why verification matters. The model has no inherent truth filter. Garbage in, garbage available.

</Question>

<Question title="Will we run out of training data?">

Possibly. High-quality text may already be scarce. Researchers explore synthetic data (model-generated), multimodal data (images, video), and data efficiency improvements. 

The "data wall" is a real concern. Larger models need more data; there may not be enough human-generated text at sufficient quality. This could limit scaling.

</Question>

**What is "loss" and why minimize it?**

**Loss** measures how wrong the model's predictions are. For LLMs, this is typically **cross-entropy loss**: roughly, how surprised was the model by the actual next token?

If the model predicted "mat" with 90% probability and the actual word was "mat," loss is low. If it predicted "mat" with 1% probability, loss is high.

Training minimizes average loss across all predictions. Lower loss means the model assigns higher probability to what actually came next. It's becoming a better predictor.

<Question title="Is cross-entropy related to information theory?">

Directly. Cross-entropy is an information-theoretic concept from Claude Shannon's work. It measures how well a predicted probability distribution matches the actual distribution. Lower cross-entropy means better predictions, less "surprise" when the true token is revealed.

Language modeling is information theory applied.

</Question>

**The compute required**

Training large models requires staggering resources:

- **GPT-3** (175B parameters): Thousands of GPU-years of computation
- **Frontier models**: Tens to hundreds of millions of dollars in compute
- **Training time**: Weeks to months on thousands of specialized chips
- **Power consumption**: Equivalent to small towns

This is why only a few organizations train frontier models. The capital requirements are prohibitive. Most practitioners use pre-trained models rather than training from scratch.

<Question title="What happens if a training run fails?">

Real money lost. Training instabilities, hardware failures, or bugs can ruin runs. Teams monitor constantly and checkpoint frequently (save intermediate states). A failed run means restarting from the last good checkpoint. Major failures (fundamental approach not working) mean writing off the investment.

This is why training is conservative: proven techniques, extensive testing before runs.

</Question>

<Expandable title="Distributed training across many machines">

No single machine can train a large model. The solution: **distributed training** across hundreds or thousands of machines.

**Data parallelism**: Each machine processes different batches, computes gradients, and they're averaged together.

**Model parallelism**: The model itself is split across machines. Different layers live on different chips.

**Pipeline parallelism**: Different stages of forward and backward passes run simultaneously on different machines.

Coordinating this requires careful engineering. After each batch, machines share gradients and synchronize parameter updates. Network bandwidth becomes a bottleneck. Machines wait for each other. Techniques like gradient compression, asynchronous updates, and clever batching minimize waiting. It's as much a distributed systems problem as a machine learning one.

</Expandable>

**Stages of training**

Modern LLMs typically go through multiple phases:

**Pre-training**: Massive text prediction on diverse data. Builds general language understanding and factual knowledge.

**Supervised fine-tuning (SFT)**: Training on curated instruction-response pairs. Teaches the model to follow instructions and produce helpful responses.

**Reinforcement learning from human feedback (RLHF)**: Training on human preferences about response quality. Aligns the model with what users actually want.

Each stage builds on the previous. Pre-training provides the foundation. Fine-tuning shapes the interface. RLHF polishes the behavior.

<Recognition title="Base vs assistant models">

If you've used both a "base" model and an "assistant" model, you've experienced this difference. Base models complete prompts as if continuing web pages or Reddit threads. The helpful assistant persona emerges from SFT and RLHF, the post-training that sculpts raw capability into something specifically useful.

</Recognition>

<Metaphor title="Quarrying and carving marble">

Pre-training is quarrying a massive block of marble. Billions of parameters start as raw stone: random numbers without pattern. Training is the chisel. Each prediction error is a tap that removes a tiny flake. Trillion taps later, something emerges. Not because the sculptor had a blueprint, but because the stone itself had latent structure, and the taps revealed it.

Fine-tuning is carving. Specialized training data sculpts the general shape into something specific: an assistant, a coder, a particular persona.

RLHF is polishing. Human feedback smooths rough edges, adjusts the expression, refines the final appearance.

The foundation must be solid. You can't polish a pebble into a masterpiece. You can't fine-tune your way to capabilities that weren't latent in the pre-trained model.

</Metaphor>

<Question title="What does RLHF do that fine-tuning doesn't?">

Supervised fine-tuning teaches from examples: "given this input, produce this output." RLHF teaches from preferences: "this response is better than that one."

Preferences can capture subtle quality differences hard to specify in examples. RLHF optimizes for human judgment of quality, not just matching specific outputs. It's more flexible but harder to control.

You need less human feedback than you'd think: thousands to tens of thousands of preference examples, not billions. A reward model learns to predict human preferences from these examples, then that model provides the training signal.

</Question>

<Question title="Can we just fine-tune forever to keep improving?">

Diminishing returns. Fine-tuning adjusts behavior but can't add capabilities the base model lacks. You can teach a model to refuse harmful requests, but not to understand physics it never learned.

Pre-training builds raw capability; fine-tuning shapes it. Eventually, you need more pre-training for new capabilities. Fine-tuning polishes; it doesn't create.

</Question>

**Why does this work?**

This is the deep mystery. Predicting next tokens sounds too simple to produce intelligence. Yet it works.

<Question title="How can 'predict the next word' teach reasoning?">

No catch. That's the remarkable part. The objective is genuinely simple. But achieving excellent prediction across *all human writing* is extremely complex.

To predict text well across all human writing, you must model the processes that generate text. Humans reason, know facts, feel emotions, follow logic. Text reflects this. To predict such text, the model must develop something like reasoning, factual knowledge, emotional understanding, logical structure.

The diversity of text (science, fiction, code, arguments) forces the model to develop diverse capabilities. Simple objective + diverse data + massive scale = complex capabilities. The complexity is in the task, not the objective.

</Question>

Whether this is "real" understanding remains debated. What's undeniable: the process produces capabilities that continually surprise us.

<TryThis>

If you have access to the OpenAI or Anthropic developer consoles, check the training compute reported for various model versions. Compare costs. Notice how each generation required dramatically more compute than the last.

</TryThis>

<Sources>
<Citation type="paper" title="Language Models are Few-Shot Learners" authors="Brown et al." source="OpenAI" url="https://arxiv.org/abs/2005.14165" year={2020} />
<Citation type="video" title="Let's build GPT: from scratch, in code" source="Andrej Karpathy" url="https://www.youtube.com/watch?v=kCc8FmEb1nY" year={2023} />
<Citation type="paper" title="Training language models to follow instructions with human feedback" authors="Ouyang et al." source="OpenAI" url="https://arxiv.org/abs/2203.02155" year={2022} />
</Sources>
