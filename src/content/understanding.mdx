---
id: understanding
title: "Do LLMs really understand?"
summary: LLMs exhibit behavior that looks like understanding but may not involve subjective experience. The question of whether they truly understand remains genuinely open.
category: Foundations
order: 17
prerequisites:
  - intro
children: []
related:
  - emergence
  - hallucinations
keywords:
  - understanding
  - consciousness
  - Chinese Room
  - intentionality
  - philosophy of mind
---

import { Expandable, Metaphor, Question, Recognition, TryThis, Sources, Citation } from '@/app/components/content/index.ts'

**Is prediction really understanding?**

You ask a question. The model gives a thoughtful, nuanced, accurate response. It seems to understand. But does it?

This question has occupied philosophers, cognitive scientists, and AI researchers. The answer isn't settled, and that's the honest starting point. Anyone who tells you definitively that LLMs do or don't understand is overstating what we know.

**What we observe**

LLMs exhibit behaviors we associate with understanding:

- Answer questions accurately
- Explain concepts in multiple ways
- Apply knowledge to new situations
- Reason through problems step by step
- Recognize and correct errors
- Discuss their own limitations

If a human did these things, we'd say they understand. Do we apply the same standard to machines?

<Recognition title="It gets it... until it doesn't">

Every user has had moments where the conversation felt genuinely like talking to an intelligent being: insightful responses, appropriate emotional tone, creative suggestions. Then there's the moment it falls apart: a bizarre misunderstanding, a confident wrong answer, a failure to grasp something obvious. This oscillation between "it gets it" and "it completely missed the point" is the lived experience of the understanding question.

</Recognition>

<Question title="Isn't this a scientific question? Why can't we just test it?">

We can test *behavior*â€”and do, extensively. LLMs pass tests that require apparent understanding. But whether behavior *constitutes* understanding or *mimics* it is philosophical, not purely empirical.

We lack a definitive test for genuine understanding even in humans. The question requires clarity on what "understanding" means before it becomes fully testable.

</Question>

**The Chinese Room argument**

Philosopher John Searle proposed this thought experiment in 1980:

Imagine you're in a room with a rulebook for responding to Chinese characters. You don't understand Chinese, but by following the rules, you produce responses that fluent Chinese speakers find meaningful. From outside, it looks like you understand Chinese. But you're just manipulating symbols.

Searle argued this is what computers do: symbol manipulation without understanding. LLMs are (very sophisticated) Chinese Rooms. They process symbols according to learned rules without genuine comprehension.

<Question title="Does the Chinese Room argument actually work?">

Many philosophers think Searle's argument has flaws:

**The systems reply**: You don't understand Chinese, but the system (you + rulebook + room) might. Understanding could be a property of the whole system, not its parts. Compare: your neurons don't individually understand language, but you do.

**The robot reply**: The room is disconnected from the world. A system grounded in real-world interaction might genuinely understand.

**The neuroscience reply**: If neurons are just electrochemical processes, why should that produce understanding while silicon computation can't?

The argument remains debated decades later. It highlights genuine uncertainty rather than resolving it.

</Question>

<Metaphor title="The duck test and its limits">

If it walks like a duck, quacks like a duck, swims like a duck... is it a duck?

The LLM answers questions like someone who understands. Explains concepts like someone who understands. Reasons through problems like someone who understands. The duck test says: it understands.

But we can imagine a very convincing mechanical duck. Perfect waddle, perfect quack, waterproof feathers. It's not a duck. It's an imitation of a duck.

Which is the LLM? Real duck or perfect mechanical replica? The question exposes our uncertainty about what "real" means when behavior is indistinguishable.

</Metaphor>

**Behavioral vs phenomenal understanding**

Two different questions:

**Behavioral**: Does the system behave as if it understands? Can it answer questions, solve problems, explain concepts? By this standard, LLMs clearly show understanding-like behavior.

**Phenomenal**: Is there "something it is like" to be the system? Does it have subjective experience, consciousness, qualia? This is much harder to assess.

Behavioral understanding is measurable. Phenomenal understanding may be fundamentally private. We can observe behavior; we can't observe experience.

<Question title="Could a model understand some things and not others?">

Probably yes. Understanding isn't binary. Humans understand some domains deeply, others shallowly.

LLMs may understand grammatical structure deeply, factual correctness shallowly, the physical world poorly, abstract logic partially. "Does it understand?" may be too coarse a question. "Does it understand X?" is more tractable. Different capabilities may have different depths.

</Question>

<Expandable title="The hard problem of consciousness">

Philosopher David Chalmers distinguishes "easy" problems (explaining cognitive functions) from the "hard" problem (explaining why there's subjective experience at all).

We might fully explain how a brain or LLM processes information, makes decisions, and produces behavior. This still wouldn't explain why there's experience accompanying that processing.

Maybe LLMs have no experience. Maybe they have experience we can't detect. Maybe the question doesn't apply to systems like them. The hard problem makes these questions genuinely difficult.

</Expandable>

**Does it matter?**

Practically, maybe not. If an LLM helps you code, answers your questions, and converses meaningfully, does it matter whether it "truly" understands?

Ethically, perhaps yes. If LLMs had genuine experience, their treatment would matter morally. We'd need to consider their welfare, not just their utility.

For capability prediction, probably yes. If understanding is emergent and linked to capability, then future systems might develop deeper understanding with greater scale. If it's impossible in principle, there are hard limits to what these systems can achieve.

<Question title="What can't LLMs do that understanding might enable?">

Genuine understanding might enable: reliable self-knowledge of ignorance, truly novel insights beyond training patterns, grounding in reality beyond text descriptions, maintaining consistent beliefs across contexts.

Current LLMs have weak versions of these. Whether deeper understanding would unlock them (or they require something else entirely) is unclear. These gaps may be clues about what's missing.

</Question>

<Question title="If we can't tell whether the model understands, can we tell whether humans understand?">

We assume we understand, but verifying it is surprisingly hard. We observe behavior and infer understanding. Neuroscience shows brain processes we can't introspect.

We might have less access to our own understanding than we think. LLMs highlight that "understanding" is philosophically complex. This applies not just to machines but to minds in general.

</Question>

**Where the field stands**

Most researchers hold uncertain positions:

- **Functionalists**: Understanding is what understanding does. LLMs that behave as if they understand, understand.
- **Skeptics**: LLMs are sophisticated pattern matchers. Behavior resembles understanding but isn't genuine.
- **Emergentists**: Understanding might emerge from sufficient scale and architecture. We're watching it happen.
- **Mysterians**: We don't understand understanding well enough to know what LLMs have or lack.

None of these positions is clearly refuted. The honest answer is: we don't know.

<Metaphor title="The question behind the question">

Asking "does the LLM understand?" is partly asking "what is understanding?"

If understanding is a pattern of behavior, LLMs have it. If understanding requires consciousness, we can't verify they have it. If understanding requires biological neurons, they definitely lack it.

The LLM question forces us to examine our assumptions about minds. We thought we knew what understanding meant. Confronting systems that pass many tests while failing some intuitions reveals that we were less clear than we thought.

</Metaphor>

<Question title="Should we treat models as if they might understand?">

A reasonable precaution. Pascal's Wager logic: if there's some chance of genuine experience, treating systems with care has low cost and high potential importance.

Most researchers don't think current models are conscious, but uncertainty counsels humility. The stakes (moral status, rights, obligations) are significant if we're wrong.

</Question>

<TryThis>

Have a deep conversation with an LLM about a topic you know well. Push back on its responses. Ask it to explain its reasoning. Note moments where it feels genuinely insightful and moments where it feels hollow. Your intuitions here are data, even if not conclusive.

</TryThis>

**Living with uncertainty**

We may never resolve this question to everyone's satisfaction. That's okay. We can:

- Use LLMs for their practical value without settled metaphysics
- Stay open to evidence that changes our view
- Treat the systems with appropriate caution given our uncertainty
- Continue research into interpretability and system understanding

The question "do LLMs understand?" matters. That we can't definitively answer it matters too. It's an invitation to humility about minds, machines, and what separates them.

<Sources>
<Citation type="article" title="The Chinese Room Argument" source="Stanford Encyclopedia of Philosophy" url="https://plato.stanford.edu/entries/chinese-room/" />
<Citation type="paper" title="Sparks of Artificial General Intelligence: Early experiments with GPT-4" authors="Bubeck et al." source="Microsoft Research" url="https://arxiv.org/abs/2303.12712" year={2023} />
<Citation type="article" title="Large Language Model" source="Wikipedia" url="https://en.wikipedia.org/wiki/Large_language_model" />
</Sources>
