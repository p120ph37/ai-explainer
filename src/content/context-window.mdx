---
id: context-window
title: "The context window"
summary: LLMs can only consider a limited amount of text at once. This "context window" is their working memory, and understanding it explains many AI behaviors.
category: Foundations
order: 3
prerequisites:
  - tokens
children:
  - attention
related:
  - vector-databases
  - memory
keywords:
  - context window
  - context length
  - memory
  - token limit
---

import { Expandable, Metaphor, Question, Recognition, TryThis, Sources, Citation } from '@/app/components/content/index.ts'
import { DiagramPlaceholder, BarChart } from '@/app/components/diagrams/index.ts'

**Why does ChatGPT sometimes "forget" what you told it earlier?**

It hasn't forgotten. It simply can't see that part of your conversation anymore.

LLMs have a **context window**: a maximum number of tokens they can consider at once. Everything the model knows about your conversation must fit within this window. Your messages, its responses, any system instructions. All of it competes for the same limited space.

When a conversation grows too long, older content gets pushed out. The model isn't storing memories elsewhere. If it's outside the window, it's gone.

<Metaphor title="A desk, not a filing cabinet">

The context window is a desk, not a filing cabinet. Everything the model thinks about must fit on this desk. Your messages, its responses, the system's instructions. All spread across the same finite surface. There are no drawers to open, no shelves to reach toward, no archives to retrieve from. If it's on the desk, the model can see it. If it's not, it doesn't exist.

When you slide a new document onto a crowded desk, something falls off the other side. The model isn't choosing to forget. There's simply no room.

Some applications try to simulate filing cabinets by summarizing old conversations and keeping the summaries on the desk. But this is a workaround, not true memory. The original details are still gone.

</Metaphor>

<Recognition title="But I already told you that">

A long conversation with an AI assistant starts going in circles, or it contradicts something it said earlier, or it asks a question you already answered. The "but I already told you that" moment, the frustration when the AI seems to ignore previous instructions, is the context window felt viscerally. It's not ignoring you; it literally cannot see that message anymore.

</Recognition>

**How big is the context window?**

It varies by model and keeps growing:

<BarChart
  title="Context Window Sizes (Tokens)"
  horizontal={true}
  data={[
    { label: 'GPT-3 (2020)', value: 4096 },
    { label: 'GPT-4 (2023)', value: 128000 },
    { label: 'GPT-5.1 (2025)', value: 256000 },
    { label: 'Claude 4.5 (2025)', value: 200000 },
    { label: 'Gemini 2.0 (2025)', value: 2000000 },
  ]}
  ariaLabel="Bar chart showing context window growth from 4K tokens in 2020 to 2M tokens in 2025"
/>

These numbers sound large, but they fill up fast. A back-and-forth conversation accumulates tokens quickly. Every message you send, every response generated, every piece of context provided by the application â€” all counts against the limit.

<Question title="What happens when you hit the limit?">

Different systems handle this differently, but something must give.

**Truncation:** The oldest messages simply get dropped. You keep chatting, but the model's view of history slides forward, forgetting the beginning.

**Summarization:** Some systems periodically compress older conversation into a shorter summary, preserving key points while freeing tokens.

**Refusal:** Some systems tell you the context is full and you need to start a new conversation.

The application you're using makes this choice, often invisibly. When an AI assistant suddenly seems to lose track of your project, it may have silently truncated your earlier context.

</Question>

<Question title="Does the model know when things get pushed out?">

No. It has no awareness of what's outside the window. It can only process what's currently in context. If you mention something from the truncated part, it can't recognize "I used to know this but don't now." It simply has no representation of absent content.

This is why AI sometimes contradicts earlier statements. Not because it disagrees, but because it can't see them.

â†’ [Why Do LLMs Make Things Up?](/hallucinations)

</Question>

**Why can't they just make it bigger?**

They're trying. Context windows have grown dramatically. But there are real constraints.

**Computational cost scales with context length.** The attention mechanism requires comparing every token to every other token. Double the context, roughly quadruple the computation.

**Quality can degrade with length.** Models trained on shorter contexts may struggle to use very long ones effectively. Research shows that information in the middle of long contexts often gets less attention than information at the beginning or end.

**Memory requirements grow.** Long contexts require storing more intermediate values. A million-token context needs substantially more GPU memory than a thousand-token one.

<Expandable title="The 'lost in the middle' problem">

This is a documented phenomenon. LLMs often attend well to the beginning (primacy) and the end (recency), but middle content can get overlooked.

It's a trained behavior, not a hardcoded bug. Training data often puts key information at beginnings and endings. The model learns these positions matter more.

Put critical information at page 50 of a 100-page document, and the model may effectively ignore it, even though it's technically within the context window. Researchers are actively working on architectures that handle long contexts more uniformly, but it remains an open challenge.

</Expandable>

<Question title="Will bigger windows solve the memory problem eventually?">

Not entirely. Larger windows help, but they don't solve everything. Computational cost still scales quadratically with length. The "lost in the middle" problem means bigger isn't always better for retrieval. And truly unbounded memory (years of conversation, lifetime context) would require different architectures entirely.

Bigger windows are a meaningful improvement, but not a permanent solution.

â†’ [How Does Attention Work?](/attention)

</Question>

<Question title="What about the training data? Doesn't the model 'remember' that?">

That's different. Training encodes patterns into weights. The model "knows" things in the sense that certain patterns are baked in. But this isn't accessible memory you can update or query. It's more like reflexes than files.

The context window is working memory: fresh, queryable, updateable within a session. Training is more like long-term knowledge that can't be modified without retraining.

â†’ [What are Parameters?](/parameters)

</Question>

<Question title="Can the model summarize its own context to 'make room'?">

Some systems do this. The model can generate summaries that compress earlier context, then continue with summary + recent messages. Others store old content in a vector database and retrieve relevant information when the conversation returns to indexed topics.

But neither is free: summarization loses information and can introduce errors; retrieval adds latency and may miss relevant context. It's a form of memory, and arguably similar to how human memory works (imperfect, associative), but it's not the eidetic recall you might expect from a machine.

</Question>

**What does this mean for how you use AI?**

Understanding context windows changes how you interact with AI tools.

**Front-load important information.** Put critical context early in your prompt where it's less likely to be truncated and more likely to receive attention.

**Be concise.** Verbose prompts waste tokens. Every unnecessary word is space that could hold useful context.

**Start fresh when needed.** If a conversation has gone on too long and the AI seems confused, starting a new conversation with a clear summary of relevant context often works better than continuing. Many users discover this folk knowledge through trial and error.

**Provide context explicitly.** Don't assume the model remembers previous conversations. Each session typically starts with an empty context window.

<TryThis>

In a long conversation, try asking the AI to summarize what it knows about your project or request. If the summary is missing key details you mentioned earlier, those details have likely fallen outside the context window. This is a useful diagnostic when the AI seems to be ignoring your requirements. Asking for a summary is also intuitive context compression, fitting more meaning into fewer tokens.

</TryThis>

<DiagramPlaceholder
  toyId="context-window-viz"
  title="Context Window Visualizer"
  description="Watch the context fill up and see what gets forgotten as conversation grows"
  icon="ðŸ“œ"
/>

**The context window shapes AI capabilities**

Many limitations people attribute to AI "intelligence" are actually context window constraints.

Can't maintain a coherent project over weeks? Context window. Contradicts earlier statements? Context window. Needs repeated reminders? Context window.

As context windows grow, some of these limitations will ease. But the fundamental constraint remains: the model can only reason about what it can currently see. There's no background knowledge store, no long-term memory, no filing cabinet. Everything happens on a desk of limited size, and when the desk fills, things fall off.

<Sources>
<Citation type="paper" title="Lost in the Middle: How Language Models Use Long Contexts" authors="Liu et al." source="Stanford" url="https://arxiv.org/abs/2307.03172" year={2023} />
<Citation type="article" title="Context window" source="Wikipedia" url="https://en.wikipedia.org/wiki/Context_window" />
<Citation type="docs" title="Claude's context window" source="Anthropic" url="https://docs.anthropic.com/en/docs/about-claude/models" />
</Sources>
