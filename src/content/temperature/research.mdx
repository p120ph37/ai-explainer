---
title: "What is Temperature in AI? — Research Voice"
summary: "Examines claims and provides supporting evidence, research, and additional context."
draft: true
---

## Core Claims and Evidence

### Claim: Temperature controls randomness in token sampling

**Technical definition:** Temperature T is applied by dividing logits by T before softmax:

```
P(token_i) = exp(logit_i / T) / Σ exp(logit_j / T)
```

**Effect:** 
- T < 1: Sharpens distribution (higher probability on top tokens)
- T = 1: No change
- T > 1: Flattens distribution (more probability on lower-ranked tokens)
- T → 0: Approaches deterministic argmax selection

**Source:** Standard softmax temperature scaling, documented in machine learning textbooks and API documentation:
- Anthropic API: https://docs.anthropic.com/en/api/messages
- OpenAI API: https://platform.openai.com/docs/api-reference

---

### Claim: Top-k and top-p are alternative/complementary sampling methods

**Top-k sampling:** Keep only the k highest-probability tokens, renormalize.

**Top-p (nucleus) sampling:** Keep smallest set of tokens whose cumulative probability ≥ p.

**Primary paper:** "The Curious Case of Neural Text Degeneration" (Holtzman et al., 2019) introduced nucleus sampling and demonstrated its advantages over pure temperature and top-k: https://arxiv.org/abs/1904.09751

**Key finding:** Pure temperature scaling allows improbable tokens to be sampled, causing "degenerate" text. Nucleus sampling truncates the distribution adaptively, preventing low-probability tail tokens while allowing natural variation.

---

### Claim: Temperature=0 is mostly but not always deterministic

**Sources of non-determinism at temperature=0:**

1. **Floating-point variance:** GPU parallel operations can produce slightly different results across runs due to non-associativity of floating-point addition.

2. **Batching effects:** If multiple requests are batched together, the batch composition can affect results.

3. **Provider-side sampling:** Some providers use internal sampling even when user-specified temperature is 0.

4. **Model updates:** API-served models may be updated silently.

**Seed parameters:** OpenAI and some other providers offer `seed` parameters for reproducibility. Documentation notes that results are "mostly deterministic" but not guaranteed identical: https://platform.openai.com/docs/api-reference/chat/create#chat-create-seed

---

### Claim: Low temperature causes repetition loops

**Phenomenon:** At very low temperatures, the model often generates repetitive text. The same phrase becomes the highest-probability continuation of itself, creating loops.

**Analysis:** "Neural Text Degeneration" (Holtzman et al., 2019) documents this as "repetition degeneration" — beam search and greedy decoding produce highly repetitive, unnatural text.

**Solution effectiveness:** The paper shows that nucleus sampling dramatically reduces repetition while maintaining coherence.

---

### Claim: Different tasks require different temperature settings

**General guidelines from provider documentation:**

Anthropic recommendations:
- 0.0-0.3: Analytical tasks, coding, fact retrieval
- 0.5-0.7: Balanced conversation, explanations
- 0.8-1.0: Creative writing, brainstorming

**Empirical notes:**
- These are heuristics, not strict rules
- Optimal temperature varies by task, model, and user preference
- A/B testing on your specific use case yields better results than following generic guidelines

---

## Additional Research and Context

### Temperature and creativity

**Research question:** Does higher temperature actually produce more "creative" output?

**Findings:** Mixed. Higher temperature increases lexical diversity and surprisal, but:
- Coherence degrades at high temperatures
- Human ratings of creativity don't monotonically increase with temperature
- "Creativity" is hard to measure objectively

**Study:** "Creative Writing with an AI-Powered Writing Assistant" (various authors) finds that moderate temperatures often rate highest for creative tasks.

### Constrained decoding

**Alternative approach:** Instead of temperature, constrain the model's outputs through structured generation (JSON mode, grammar-constrained decoding).

**Benefit:** Guarantees structural validity while using normal sampling for content.

**Implementations:**
- OpenAI JSON mode
- Anthropic tool use
- Open-source: guidance, LMQL, outlines

### Temperature in different model families

**Observation:** The "feel" of different temperature values varies across models. Temperature 0.7 on GPT-4 might produce different diversity than 0.7 on Claude.

**Reason:** Temperature operates on the raw logit distribution, which differs by model architecture and training. There's no universal calibration.

**Implication:** Temperature settings that work for one model may need adjustment for another.

### Adaptive temperature

**Research direction:** Automatically adjusting temperature based on context or token position.

**Idea:** Use lower temperature for tokens that need to be precise (names, numbers) and higher temperature for tokens that can vary (adjectives, phrasing).

**Status:** Active research area; not yet standard in production systems.

---

## Recommended Resources

**For understanding:**
1. Holtzman et al. "Neural Text Degeneration" (2019) — foundational paper on sampling problems and solutions
2. Hugging Face blog "How to generate text": https://huggingface.co/blog/how-to-generate
3. API documentation from Anthropic/OpenAI

**For implementation:**
1. Provider API references for parameter details
2. Hugging Face transformers `generate()` documentation for technical details

**For experimentation:**
1. OpenAI Playground (adjust temperature in real-time)
2. Anthropic Console (compare outputs at different settings)
3. Local models via llama.cpp or similar (full parameter control)
