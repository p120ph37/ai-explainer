---
id: emergence
title: "What is emergent behavior?"
summary: Emergence is when simple rules produce complex behavior. In LLMs, capabilities like reasoning appear spontaneously at scale, not from explicit programming.
category: Foundations
order: 3
prerequisites:
  - intro
children: []
related:
  - scale
  - training
  - neural-network
  - understanding
keywords:
  - emergence
  - emergent capabilities
  - phase transitions
  - complexity
---

import { Expandable, Metaphor, Question, Recognition, TryThis, Sources, Citation } from '@/app/components/content/index.ts'
import { GameOfLife } from '@/app/components/diagrams/index.ts'

**Why do capabilities suddenly appear at certain scales?**

You might expect AI capabilities to improve smoothly as models get bigger. Instead, some abilities are absent in smaller models, then suddenly present in larger ones. Like flipping a switch.

This is **emergence**: capabilities that arise from scale without being explicitly programmed. The model wasn't trained to do arithmetic. Yet at sufficient size, it can. Nobody taught it to translate between languages it rarely saw paired. Yet it does.

<Metaphor title="When quantity becomes quality">

Heating water teaches nothing about steam. At 99°C, you have hot water. At 100°C, you suddenly have something qualitatively different: gas instead of liquid, expansion instead of stability, new behaviors that weren't present moments before.

This is a phase transition. The rules didn't change. The temperature increased by one degree. But the system crossed a threshold where its fundamental behavior transformed.

LLMs undergo similar transitions. A small model is autocomplete. A larger model is something else: a system that reasons, explains, creates. More of the same became something new.

</Metaphor>

<Recognition title="Wait, it can do THAT?">

"GPT-4 passes the bar exam" made headlines: capabilities appearing that surprised even developers. Or the first time you asked an AI something you didn't expect it to handle, and it succeeded. That "wait, it can do THAT?" surprise is emergence felt firsthand.

</Recognition>

**Emergence beyond AI**

This phenomenon isn't unique to language models. It appears throughout nature and mathematics:

- **Water**: Individual H₂O molecules don't have wetness. But enough molecules together are wet.
- **Flocking**: Each bird follows simple rules. The flock exhibits complex, coordinated patterns.
- **Consciousness**: Individual neurons aren't conscious. Yet somehow, enough neurons together produce experience.
- **Cities**: No central planner designs traffic patterns. They emerge from individual decisions.

In each case, the whole exhibits properties absent from the parts. Simple components, simple rules, complex outcomes.

<Question title="How does scale enable emergence?">

Emergence in LLMs is tied to **scale**: more parameters, more training data, more compute. Researchers discovered that model capabilities improve predictably as you scale up, following mathematical "scaling laws."

But while improvement is predictable, *which* capabilities emerge isn't. Below certain scale thresholds, a capability is absent. Above it, the capability appears, sometimes suddenly. The scaling laws tell us loss will improve; they don't tell us what new abilities will crystallize at each threshold.

→ [Why scale matters](/scale)

</Question>

<Question title="Isn't 'emergence' just admitting we don't understand?">

Calling something emergent describes the phenomenon (appears at scale, not programmed) without explaining mechanisms. But emergence is a real, studied pattern in complex systems. It's not just ignorance with a fancy name.

We do have working theories being actively pursued: larger models may have capacity to store not just word associations but deeper concept associations, enabling qualitatively different behavior. Understanding *that* capabilities emerge is progress; understanding *why* is an active research area making steady advances.

→ [Do LLMs Really Understand?](/understanding)

</Question>

**Emergent capabilities in LLMs**

Documented emergent capabilities include:

- **Multi-step arithmetic**: Small models can't add three-digit numbers. Large models can.
- **Chain-of-thought reasoning**: The ability to work through problems step by step.
- **Cross-lingual transfer**: Learning a skill in one language, applying it in another.
- **Theory of mind**: Modeling what other agents might believe or want.
- **Code execution tracing**: Mentally stepping through code to predict outputs.

These capabilities appear at different scale thresholds. Below the threshold: failure. Above it: success. The transition can be sharp.

<Question title="Is emergence real or a measurement artifact?">

This is debated. Some researchers argue emergent capabilities are artifacts of how we measure. If we used different metrics, we might see gradual improvement instead of sudden jumps.

Others point to genuine phase transitions: qualitative changes in what the model can represent. Like water freezing, there may be critical points where system behavior fundamentally shifts.

The debate matters practically. If emergence is a real phase transition, there may be fundamental scales where capabilities unlock. If it's a measurement artifact, capabilities might improve smoothly and we just measure them poorly. This affects how we forecast future models and plan for safety.

</Question>

<Question title="How does cross-lingual transfer work if pairs were rarely seen together?">

If a model has developed internal, language-agnostic representations of concepts, translation becomes: input language → internal conceptual representation → output language. The model learned translation structure from common pairs and learned the rare language from other contexts. It can combine these, translating pairs it barely saw together by routing through its internal concept space.

Emergence happens when skills learned in one context apply to new combinations.

→ [Why Does Scale Matter?](/scale)

</Question>

**Why does prediction create reasoning?**

This is the core mystery. The training objective is simple: predict the next token. How does this create capabilities like reasoning or planning?

One hypothesis: to predict well across diverse text, you must model the processes that generate text. Humans reason, plan, and know facts. Text reflects this. To predict text well, the model must develop something like reasoning, planning, and factual knowledge.

<Metaphor title="The game that teaches everything">

Imagine a game where you must predict what any person will say next, given any context, across every conversation ever held.

To win this game, you must understand:
- Grammar (to predict syntactically valid sentences)
- Facts (to predict accurate statements)
- Logic (to predict valid arguments)
- Emotions (to predict human responses)
- Intent (to predict where conversations go)

This one game, played at sufficient scale, teaches everything needed to generate human-like language. Not because the game is complex, but because human language is complex, and mastering the game requires mastering what generates language.

</Metaphor>

The prediction task is simple. But *achieving* good prediction on all human text is not simple. It requires modeling the full complexity of human thought. LLMs are better at tasks heavily documented in text than purely physical ones, partly because humans writing for other humans assume a baseline of embodied physical understanding that LLMs lack.

<TryThis>

Explore Conway's Game of Life below. Watch patterns evolve from the starting configuration. See gliders emerge and move across the grid. Notice how complexity arises without being designed. The four simple rules say nothing about movement, yet things move. This is emergence in visible action. LLM emergence is similar but harder to see: complex capabilities emerging from simple gradient updates.

</TryThis>

<GameOfLife
  title="Conway's Game of Life"
  initialWidth={32}
  initialHeight={24}
  initialPreset="glider"
/>

**Emergence and unpredictability**

Emergence is partly why AI capabilities are hard to forecast. We can predict loss improvements from scaling laws. We cannot easily predict *which capabilities* will emerge at which scales.

This makes frontier AI development partly exploratory. You build bigger models partly to discover what they can do. The capabilities weren't specified in advance; they're found empirically.

<Question title="If we can't predict emergence, how can we say AI is safe?">

This is a core concern in AI safety research. Unpredictable emergence means we might discover dangerous capabilities post-deployment. Current approaches: extensive testing, staged releases, monitoring for harmful behaviors, building in safeguards.

But the fundamental unpredictability is genuine, which is why capability forecasting and alignment research are active fields. We can't fully predict emergence, so we test extensively after the fact.

</Question>

<Question title="Can you train a model NOT to have certain emergent capabilities?">

It's complicated. You can't prevent capabilities from emerging if they arise from scale. You can train the model not to *demonstrate* them (refuse certain requests), but the underlying capability may still exist.

This is the capability vs. behavior distinction. Safety work focuses on behavior modification, but capabilities remain latent. How to truly prevent harmful capabilities from emerging is an open research problem.

</Question>

<Question title="Does 'theory of mind' emergence mean the model understands what we're thinking?">

It means the model can predict text about mental states: what characters believe, want, might do. It performs well on tasks that test this. Whether this constitutes genuine understanding of minds or sophisticated pattern matching on mind-related text is debated.

The capability is real; its interpretation is contested.

→ [Do LLMs Really Understand?](/understanding)

</Question>

**What should you take away?**

Complex capabilities can arise from simple objectives applied at scale. Predicting text well enough requires modeling the world. We can't fully predict what will emerge as models grow.

Emergence is the bridge between simple training objectives and surprising capabilities. It's both the source of LLMs' impressive abilities and the reason their development contains genuine uncertainty.

<Sources>
<Citation type="paper" title="Emergent Abilities of Large Language Models" authors="Wei et al." source="Google Research" url="https://arxiv.org/abs/2206.07682" year={2022} />
<Citation type="paper" title="Are Emergent Abilities of Large Language Models a Mirage?" authors="Schaeffer et al." source="Stanford" url="https://arxiv.org/abs/2304.15004" year={2023} />
<Citation type="article" title="Emergence" source="Wikipedia" url="https://en.wikipedia.org/wiki/Emergence" />
</Sources>
