---
id: intro
title: What is an LLM?
summary: Large Language Models are AI systems that have learned to predict and generate text by studying vast amounts of human writing.
prerequisites: []
children:
  - tokens
  - why-large
  - what-ai-is
related:
  - how-different-from-search
keywords:
  - LLM
  - large language model
  - AI
  - ChatGPT
  - Claude
---

import { Expandable, Metaphor, Question, Recognition, TryThis, Sources, Citation, Footnote } from '../../app/components/content/index.ts'

**What actually happens when you talk to ChatGPT?**

When you type a message and hit send, the system receives your text and does something that sounds almost disappointingly simple: it predicts what text should come next.

That's it. That's the core of what a **Large Language Model** (LLM) does. Given some text, predict what text would naturally follow.

**If it's just predicting text, why does it seem to understand things?**

This is perhaps the most fascinating part. The model was trained by showing it enormous amounts of human writing (books, websites, conversations, code) and asking it, over and over: "What comes next?"

To predict well across all that diverse text, the model had to develop something resembling comprehension. Consider: to predict how a mystery story continues, you need to track clues and suspects. To predict the next line of code, you need to grasp what the code is doing. To predict how a physics explanation continues, you need to follow the logical thread.

The prediction task, at sufficient scale, requires understanding.

<Metaphor title="The map that draws itself">

You type a message, and the model sees your words as a coastline: the beginning of a territory. Its job is to continue the map. "If this is the shore we're starting from, what landscape extends inland?"

It draws the next bit of terrain, considers what that implies, draws more. Word by word, your conversation extends into territory that, until that moment, didn't exist.

The model isn't retrieving pre-written answers like pulling books from shelves. It's generating new territory based on the patterns of all the maps it's ever seen.

→ [How token-by-token generation works](#/tokens)

</Metaphor>

<Question title="Is prediction really the same as understanding?">

This remains genuinely open, not just politely unresolved.

What we observe: LLMs can answer questions, explain concepts, solve problems, and hold coherent conversations. They behave as though they understand.

What we don't know: whether there's "something it is like" to be an LLM processing a query. Whether the understanding is genuine or a very sophisticated imitation.

Some researchers argue this distinction may not even be meaningful. Understanding *is* a pattern of behavior, they say, and by that standard LLMs understand. Others maintain that real comprehension requires something these systems lack.

The honest answer: we're not sure, and that's okay. What matters for practical purposes is knowing what the capabilities are and where they fail.

→ [The AI understanding debate](#/understanding)

</Question>

**Why "Large"? What makes these different from the autocorrect on my phone?**

Scale creates qualitative change.

Your phone's predictive text uses a small model with limited context. It considers maybe a sentence and suggests common words. An LLM uses a model with billions of parameters, considers thousands of words of context, and has absorbed patterns from trillions of words of training data.

<Question title="What are 'parameters' and why so many?">

Parameters are the learned values in the neural network: numbers adjusted during training to make predictions more accurate. More parameters means more capacity to store patterns and make fine-grained distinctions.

GPT-3 has 175 billion parameters.<Footnote id={1} /> GPT-5.1 is estimated to have over two trillion. For comparison: a typical smartphone keyboard predictor might have a few million. The jump from millions to trillions isn't just "bigger." It's the difference between suggesting the next word and writing a coherent essay on quantum physics.

Training these models requires thousands of specialized chips running for months, consuming electricity equivalent to small towns. The "large" in Large Language Model isn't hyperbole.

→ [Inside the neural network](#/architecture)

</Question>

The difference isn't just "more of the same." At sufficient scale, new capabilities appear that weren't present in smaller models. A small model might complete "The capital of France is" with "Paris." A large model can write you a detailed historical analysis of Parisian urban development, synthesizing information it never saw combined in training.

<Recognition>

**You've experienced something like this.** After reading dozens of mystery novels, you start predicting plot twists. Not because you memorized them, but because you've internalized patterns. LLMs have done this with far more text than any human could read in a lifetime.

</Recognition>

<Metaphor title="When quantity becomes quality">

A small prediction engine is just autocomplete, useful for finishing "See you tom-" with "tomorrow."

Increase the scale dramatically and something qualitative shifts. It's like heating water: at 99°C you still have water, at 100°C you suddenly have steam. More parameters, more training data, more compute, and suddenly the model can write essays, debug code, engage in philosophical discussion.

Researchers call these **emergent capabilities**.<Footnote id={2} /> They weren't programmed in; they crystallized from scale. Nobody told the model how to summarize or translate or reason about ethics. These abilities appeared as side effects of learning to predict text at sufficient depth.

→ [Emergent capabilities and scaling laws](#/emergence)

</Metaphor>

**How does it generate long, coherent responses?**

One word at a time. Or more precisely, one "token" at a time (tokens are typically words or word-pieces).

The model generates a probability distribution: "Given everything so far, what token is most likely next?" It samples from that distribution, adds the chosen token to the context, and repeats. Your multi-paragraph response emerged token by token, each one conditioned on everything before it.

<TryThis>

**Watch the generation happen:** Some interfaces stream responses as they're generated. You can watch the text appear word by word. Try asking the same question twice and notice that you'll get different responses. The sampling process has randomness built in, so the same prompt can lead to different outputs.

</TryThis>

<Question title="Why add randomness? Shouldn't it give the 'best' answer?">

You might expect a computer to always produce the same "most correct" output. But for language, there usually isn't one correct continuation. Ask someone "How was your weekend?" and there are thousands of valid responses. The model faces the same situation at every word.

Without randomness, the model would always pick the single highest-probability next word. This sounds ideal but causes problems. The output becomes repetitive and mechanical. Worse, the model can get trapped in loops, repeating the same phrases because they keep being the "most likely" continuation of themselves.

Controlled randomness lets the model explore the space of reasonable responses. Instead of always choosing the top prediction, it sometimes picks the second or third most likely option. The result feels more natural, more varied, more like how a thoughtful person might answer the same question differently on different days.

You can often adjust this randomness (called "temperature") in AI interfaces. Lower temperature means more predictable, focused responses. Higher temperature means more creative, surprising ones.

→ [Sampling and temperature](#/sampling)

</Question>

Understanding how LLMs work changes how you use them, and how you evaluate claims about them.

When someone says an LLM "knows" something, you now understand: it has learned patterns around that concept. When it makes a confident mistake, you understand why: it predicted a plausible-sounding continuation that happened to be false. When new capabilities emerge, you can place them in context: better prediction enabling new behaviors.

You're not working with a magical oracle or a search engine with personality. You're collaborating with a sophisticated pattern-matcher that has absorbed more human writing than any person could read in a thousand lifetimes.

<Sources>
<Citation type="video" title="But what is a GPT? Visual intro to transformers" source="3Blue1Brown" url="https://www.youtube.com/watch?v=wjZofJX0v4M" year={2024} />
<Citation type="video" title="Interpretability: Understanding how AI models think" source="Anthropic" url="https://www.youtube.com/watch?v=fGKNUvivvnc" year={2025} />
<Citation type="article" title="Tracing the thoughts of a large language model" source="Anthropic" url="https://www.anthropic.com/research/tracing-thoughts-language-model" year={2025} />
<Citation type="article" title="Large language model" source="Wikipedia" url="https://en.wikipedia.org/wiki/Large_language_model" />
<Citation type="paper" title="Language Models are Few-Shot Learners [1]" authors="Brown et al." source="OpenAI" url="https://arxiv.org/abs/2005.14165" year={2020} />
<Citation type="paper" title="Emergent Abilities of Large Language Models [2]" authors="Wei et al." source="Google Research" url="https://arxiv.org/abs/2206.07682" year={2022} />
</Sources>
