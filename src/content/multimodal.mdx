---
id: multimodal
title: "How do LLMs see images?"
summary: Multimodal models process images, audio, and video alongside text. They convert everything into tokens, allowing the same architecture to reason across modalities.
category: Foundations
order: 25
prerequisites:
  - intro
  - tokens
  - embeddings
  - transformer
children: []
related:
  - tokens
  - embeddings
  - training
keywords:
  - multimodal
  - vision
  - images
  - audio
  - GPT-4V
  - CLIP
---

import { Expandable, Metaphor, Question, Recognition, TryThis, Sources, Citation } from '@/app/components/content/index.ts'

**How can a language model understand pictures?**

Early LLMs only processed text. Modern multimodal models handle images, audio, video, and more. The trick: convert everything into the same format the model already understands, tokens and embeddings.

An image becomes a sequence of visual tokens. Audio becomes a sequence of audio tokens. The transformer processes them just like text, learning relationships between modalities.

**Images as tokens**

Text is tokenized into words and subwords. Images are tokenized into patches.

An image is divided into a grid (say, 16×16 patches). Each patch becomes a token with its own embedding. A 256×256 image with 16×16 patches produces 256 visual tokens.

These visual tokens enter the same transformer that processes text tokens. The model learns to connect "dog" (text) with patches showing a dog (image).

```
Image: [patch_1] [patch_2] [patch_3] ... [patch_256]
Text:  [What] [animal] [is] [this] [?]
Combined: [patch_1] ... [patch_256] [What] [animal] [is] [this] [?]
```

The model attends across all tokens, finding which image patches are relevant to which words.

<Recognition title="Uploading images to ChatGPT">

You've used this if you've uploaded an image to ChatGPT or Claude and asked questions about it. The model isn't running separate image analysis. It's processing image patches and text together, understanding their relationships.

</Recognition>

**Vision encoders**

Converting images to useful tokens requires a vision encoder, a neural network that understands visual content.

Common approaches:

**CLIP-style**: Train an image encoder and text encoder together. Images and their descriptions end up in the same embedding space. "A photo of a cat" and an actual cat photo have similar vectors.

**ViT (Vision Transformer)**: Treat image patches like text tokens. Apply transformer architecture directly to images. Pre-train on massive image datasets.

**Connector modules**: Bridge between frozen vision encoders and language models. Map visual representations into the LLM's token space.

The vision encoder does the heavy lifting of understanding pixels. The LLM does reasoning about what it sees.

<Question title="Why not train on images from scratch?">

Language models benefit from architectural and conceptual transfer. Text transformers already understand:
- Sequences and attention
- Concepts and relationships
- Reasoning and instruction-following

Adding vision is easier than building multimodal from scratch. Use proven vision encoders, connect them to proven language models, fine-tune together.

This also enables using the same base LLM for text-only tasks (where it excels) and multimodal tasks (with vision added on).

</Question>

**Audio and speech**

Audio works similarly:

1. **Waveform to spectrogram**: Convert raw audio to frequency representations
2. **Chunk into frames**: Divide into short time segments  
3. **Encode to tokens**: Neural encoder produces audio tokens
4. **Process with LLM**: Same transformer handles audio alongside text

This enables:
- Speech transcription (audio → text)
- Voice understanding (audio + text → response)
- Audio generation (text → audio)

Models like Whisper specialize in audio-to-text. GPT-4o processes audio natively alongside text and images.

<Expandable title="Video as a modality">

Video is image sequences plus audio plus time:

- Sample frames from video
- Tokenize each frame as image patches
- Add temporal position information
- Include audio tokens if present
- Process the combined sequence

Challenges multiply:
- Token counts explode (minutes of video = millions of tokens)
- Temporal relationships matter
- Compute requirements are massive

Current video models typically sample sparse frames or summarize. Full video understanding at LLM-scale remains frontier research.

</Expandable>

**Training multimodal models**

Training combines data from multiple modalities:

**Image-text pairs**: Photos with captions, diagrams with explanations
**Interleaved documents**: Web pages with images and text mixed
**Instruction data**: "What's in this image?" → description
**Cross-modal tasks**: Describe images, answer questions about charts, follow visual instructions

The model learns to connect modalities: that "red" in text corresponds to certain pixel values, that a graph line going up means "increasing."

<Metaphor title="Learning a new sense">

Teaching vision to a language model is like a blind person gaining sight.

They already understand the world through language and concepts. They know what "red" means, what "above" means, what a "cat" is. The new visual sense doesn't replace their conceptual understanding. It adds a new input channel.

Visual tokens flow into the same brain that processes language. The model connects what it sees to what it already knows. It doesn't start over; it extends.

</Metaphor>

**What multimodal enables**

With multiple modalities:

- **Document understanding**: Read PDFs with charts, diagrams, and text together
- **Visual reasoning**: Analyze images, answer questions about what's shown
- **Creative work**: Understand and generate visual content
- **Robotics and embodiment**: Process camera feeds, sensor data
- **Accessibility**: Describe images for visually impaired users

The same reasoning capabilities that work for text now work for any tokenizable input.

<TryThis>

Upload an image with text in it (a sign, a document, a screenshot) to a multimodal AI. Ask it to read the text, describe the layout, and explain the content. Notice how it handles both the visual structure and the textual content together. That's multimodal processing in action.

</TryThis>

**Current limitations**

Multimodal models aren't perfect:

- **Spatial reasoning**: Struggle with precise locations, counts, small details
- **OCR errors**: May misread text in images
- **Hallucination**: Can "see" things that aren't there
- **Resolution limits**: May miss fine details due to patch size
- **Compute cost**: Visual tokens are expensive; long videos are prohibitive

Progress is rapid, but modalities beyond text remain harder. Text is the native language; other modalities are learned translations.

<Sources>
<Citation type="paper" title="Learning Transferable Visual Models From Natural Language Supervision (CLIP)" authors="Radford et al." source="OpenAI" url="https://arxiv.org/abs/2103.00020" year={2021} />
<Citation type="paper" title="An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT)" authors="Dosovitskiy et al." source="Google" url="https://arxiv.org/abs/2010.11929" year={2020} />
<Citation type="docs" title="Vision with Claude" source="Anthropic" url="https://docs.anthropic.com/en/docs/build-with-claude/vision" />
</Sources>
