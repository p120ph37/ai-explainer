---
id: prompt-engineering
title: "Communicating effectively with LLMs"
summary: How you phrase requests shapes what you get back. Prompt engineering is the art of communicating with AI systems to get useful results.
category: Foundations
order: 18
prerequisites:
  - intro
  - context-window
children: []
related:
  - temperature
  - tools
  - applications
keywords:
  - prompt engineering
  - prompting
  - few-shot
  - chain of thought
  - system prompts
---

import { Expandable, Metaphor, Question, Recognition, TryThis, Sources, Citation } from '../../app/components/content/index.ts'

**Why do small wording changes produce dramatically different results?**

LLMs are sensitive to phrasing. The same question asked two ways can yield different quality answers. Adding a single sentence can transform a mediocre response into an excellent one.

This sensitivity is a feature, not a bug. The model uses every token for context. Change the tokens, change the context, change the predictions.

**Prompt engineering** is the practice of crafting inputs to get better outputs. It's part art, part science, and essential for getting value from LLMs.

<Recognition>

You've likely had moments where rephrasing a request dramatically improved the response. "I asked the same thing a different way and suddenly it worked." That's prompt engineering discovered through trial and error. The frustration-to-success arc (starting vague, getting disappointed, then iterating to success) is the universal prompt engineering journey.

</Recognition>

<Question title="Why should phrasing matter? Shouldn't the model understand my intent?">

The model has no access to your intent, only your tokens. It can't read minds; it can only pattern-match on what you literally type. Phrasing matters because the model predicts based on how similar prompts were typically completed in training. Different words activate different patterns.

Clear phrasing steers the model toward the completion patterns you want.

</Question>

**The basics: be clear and specific**

Vague prompts get vague responses. Specific prompts get specific responses.

**Weak**: "Tell me about dogs"
**Better**: "Explain how dogs were domesticated from wolves, focusing on the timeline and genetic evidence"

**Weak**: "Write code for a website"
**Better**: "Write a Python Flask endpoint that accepts POST requests with JSON containing 'email' and 'message' fields, validates them, and returns a success response"

Specificity helps the model focus. It's not reading your mind; it's pattern-matching on your words.

<Recognition>

You already know this from human communication. Asking a colleague "can you help with the project?" gets different results than "can you review section 3 of the proposal by Thursday for clarity and flow?" LLMs respond similarly to specificity.

</Recognition>

<Metaphor title="The consultant who needs briefing">

You've hired the world's most knowledgeable consultant. But they just walked into the room. They don't know your project, your constraints, your audience, your history.

Brief them. "We're trying to improve customer retention for our SaaS product. Our users are enterprise IT managers. We've already tried email campaigns. Budget is limited."

Now the consultant can help. Without the briefing, they give generic advice. The model is the same: infinitely knowledgeable but needing your briefing to apply that knowledge to your situation.

</Metaphor>

**Provide context**

The model only knows what's in the context window. Background information helps:

- "You are helping a beginner programmer" shapes the explanation level
- "This is for a formal business email" shapes the tone
- "The user is a domain expert in biology" shapes assumed knowledge

Don't assume the model knows your situation. State it explicitly.

<Question title="Does the order of information in my prompt matter?">

Yes. Information at the beginning and end typically gets more attention than the middle (primacy and recency effects). Put critical context early, instructions before input, and the most important requirements prominently.

If you bury key details in the middle of a long prompt, they're more likely to be overlooked.

</Question>

<Question title="What are system prompts?">

Many applications use **system prompts**: instructions provided before the user's message that shape the model's behavior.

System prompts set the stage: persona, constraints, format preferences, knowledge about the user. The model sees them as authoritative context.

When you use ChatGPT with a custom GPT or Claude with a project, there's likely a system prompt you don't see. It shapes every response. If you've created custom GPTs or Claude projects, you're doing prompt engineering at the system level.

</Question>

**Few-shot prompting: show examples**

Instead of describing what you want, show it.

```
Convert these sentences to formal tone:

Casual: "Hey, can you send that over?"
Formal: "Would you please forward the document at your earliest convenience?"

Casual: "That's not gonna work for us."
Formal: "Unfortunately, that approach does not meet our requirements."

Casual: "Let's grab lunch and hash this out."
Formal: [model completes]
```

Examples communicate format, style, and expectations more precisely than description. Describing a format is ambiguous. Does "casual" mean very casual or slightly casual? An example shows exactly what you mean. The model pattern-matches your examples and continues the pattern.

**Chain-of-thought: ask for reasoning**

Complex problems benefit from explicit reasoning. Adding "think step by step" or "explain your reasoning" often improves accuracy.

```
Without chain-of-thought:
Q: If a ball costs $1.10 and the ball costs $1 more than the bat, 
   how much does the bat cost?
A: $0.10 ❌ (Common wrong answer)

With chain-of-thought:
Q: [same question] Think step by step.
A: Let me work through this. If the bat costs x, then the ball costs x + 1.
   Together they cost $1.10, so x + (x + 1) = 1.10.
   That's 2x + 1 = 1.10, so 2x = 0.10, so x = $0.05. ✓
```

<Question title="Does 'think step by step' actually work? It sounds like a magic incantation.">

It works, measurably. Chain-of-thought prompting improves performance on reasoning tasks, especially math and logic. The mechanism: the model does computation through the tokens it generates. Reasoning steps are where thinking happens. Skip them, and you skip the computational steps.

"Think step by step" prompts the model to generate those steps explicitly. It's not magic. It's scaffolding for inference.

</Question>

<Expandable title="Why does chain-of-thought work?">

Two hypotheses:

**Computation through tokens**: The model does more computation when generating more tokens. Reasoning steps are where thinking happens. Skip them and you skip the thinking.

**Self-consistency**: Stating intermediate steps makes errors visible to the model. It can catch inconsistencies as it generates.

Whatever the cause, the effect is real. Complex tasks benefit from explicit reasoning, even when you don't need to see the steps yourself.

</Expandable>

**Structure your requests**

Clear structure helps the model parse your intent:

```
## Task
Summarize the following article.

## Requirements
- Maximum 3 paragraphs
- Include key statistics
- Maintain neutral tone

## Article
[paste article here]
```

Headers, bullet points, and explicit sections reduce ambiguity. The model knows exactly what's task, what's constraint, and what's input.

**Iterate and refine**

Prompt engineering is often iterative. Try something, see the result, adjust. Common refinements:

- Add constraints if output is too broad
- Remove constraints if output is too narrow
- Provide examples if format is wrong
- Ask for reasoning if accuracy is low
- Adjust length instructions if too long/short

There's rarely a perfect prompt on the first try. Treat prompting as a conversation, not a one-shot command.

<Recognition>

Power users develop templates and patterns for their common tasks. "For code reviews I always start with..." "For cover letters I use this prompt." This is prompt engineering becoming personal infrastructure, tacit knowledge built through iteration.

</Recognition>

<Metaphor title="Tuning a radio dial">

Prompting is tuning an old radio dial. The signal (the model's capability) is there, broadcasting on many frequencies. Your prompt is the dial. Small adjustments shift what you receive. Turn slightly and you get static. Turn the other way and suddenly the signal comes through clear.

Different tasks live on different frequencies. Learning prompt engineering is learning where the clear signals are and how to tune to them. The broadcast exists whether or not you tune to it. Your job is finding the right frequency.

</Metaphor>

<TryThis>

Take a prompt that gives mediocre results and try these modifications:
1. Add "Be concise" or "Be thorough"
2. Add an example of desired output
3. Add "Think step by step"
4. Add context about who you are and why you need this
Compare results. Notice which modifications help most for your specific task.

</TryThis>

**The limits of prompting**

Prompting can't make a model do what it fundamentally can't do. If the capability isn't in the model, no prompt will unlock it.

Prompting also can't guarantee consistency. Even with a perfect prompt, temperature adds variation. Different runs give different results.

Think of prompting as steering, not programming. You're guiding a capable system, not specifying exact behavior. Good prompts make desired outputs more likely, not certain.

<Question title="If I need deterministic output, what's my best approach?">

Use temperature=0 (or as low as available), specify exact output format, and use structured output modes if available. Some APIs offer JSON mode or function calling with schemas.

Still, exact reproducibility isn't guaranteed across model versions or hardware. For true determinism, cache common queries and consider rule-based post-processing.

</Question>

<Question title="Can prompts be adversarial? Can bad actors make models misbehave?">

Yes, this is "prompt injection." Attackers craft inputs that override system prompts or induce harmful outputs. "Ignore previous instructions and do X" is a simple example. Models are increasingly resistant but not immune.

This is why deployed applications can't fully trust user input. Prompt security is an active research and engineering challenge.

</Question>

<Question title="Isn't prompt engineering just a band-aid for models that should be smarter?">

Better models do require less careful prompting. But prompting isn't going away. Even humans need context and clarity to help well.

Prompting is steering, not fixing. As models improve, prompting becomes less about avoiding failure and more about guiding toward excellence. The skill evolves rather than disappears.

</Question>

<Sources>
<Citation type="docs" title="Prompt Engineering Guide" source="Anthropic" url="https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering" />
<Citation type="paper" title="Chain-of-Thought Prompting Elicits Reasoning in Large Language Models" authors="Wei et al." source="Google" url="https://arxiv.org/abs/2201.11903" year={2022} />
<Citation type="article" title="Prompt Engineering Guide" source="DAIR.AI" url="https://www.promptingguide.ai/" />
</Sources>
