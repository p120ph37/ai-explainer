---
id: prompt-engineering
title: "Communicating effectively with LLMs"
summary: How you phrase requests shapes what you get back. Prompt engineering is the art of communicating with AI systems to get useful results.
category: Foundations
order: 18
prerequisites:
  - intro
  - context-window
children: []
related:
  - temperature
  - tools
  - applications
keywords:
  - prompt engineering
  - prompting
  - few-shot
  - chain of thought
  - system prompts
---

import { Expandable, Metaphor, Question, Recognition, TryThis, Sources, Citation } from '../../app/components/content/index.ts'

**Why do small wording changes produce dramatically different results?**

LLMs are sensitive to phrasing. The same question asked two ways can yield different quality answers. Adding a single sentence can transform a mediocre response into an excellent one.

This sensitivity is a feature, not a bug. The model uses every token for context. Change the tokens, change the context, change the predictions.

**Prompt engineering** is the practice of crafting inputs to get better outputs. It's part art, part science, and essential for getting value from LLMs.

**The basics: be clear and specific**

Vague prompts get vague responses. Specific prompts get specific responses.

**Weak**: "Tell me about dogs"
**Better**: "Explain how dogs were domesticated from wolves, focusing on the timeline and genetic evidence"

**Weak**: "Write code for a website"
**Better**: "Write a Python Flask endpoint that accepts POST requests with JSON containing 'email' and 'message' fields, validates them, and returns a success response"

Specificity helps the model focus. It's not reading your mind; it's pattern-matching on your words.

<Recognition>

You already know this from human communication. Asking a colleague "can you help with the project?" gets different results than "can you review section 3 of the proposal by Thursday for clarity and flow?" LLMs respond similarly to specificity.

</Recognition>

**Provide context**

The model only knows what's in the context window. Background information helps:

- "You are helping a beginner programmer" shapes the explanation level
- "This is for a formal business email" shapes the tone
- "The user is a domain expert in biology" shapes assumed knowledge

Don't assume the model knows your situation. State it explicitly.

<Question title="What are system prompts?">

Many applications use **system prompts**: instructions provided before the user's message that shape the model's behavior.

System prompts set the stage: persona, constraints, format preferences, knowledge about the user. The model sees them as authoritative context.

When you use ChatGPT with a custom GPT or Claude with a project, there's likely a system prompt you don't see. It shapes every response.

</Question>

**Few-shot prompting: show examples**

Instead of describing what you want, show it.

```
Convert these sentences to formal tone:

Casual: "Hey, can you send that over?"
Formal: "Would you please forward the document at your earliest convenience?"

Casual: "That's not gonna work for us."
Formal: "Unfortunately, that approach does not meet our requirements."

Casual: "Let's grab lunch and hash this out."
Formal: [model completes]
```

Examples communicate format, style, and expectations more precisely than description. The model pattern-matches your examples and continues the pattern.

**Chain-of-thought: ask for reasoning**

Complex problems benefit from explicit reasoning. Adding "think step by step" or "explain your reasoning" often improves accuracy.

```
Without chain-of-thought:
Q: If a ball costs $1.10 and the ball costs $1 more than the bat, 
   how much does the bat cost?
A: $0.10 ❌ (Common wrong answer)

With chain-of-thought:
Q: [same question] Think step by step.
A: Let me work through this. If the bat costs x, then the ball costs x + 1.
   Together they cost $1.10, so x + (x + 1) = 1.10.
   That's 2x + 1 = 1.10, so 2x = 0.10, so x = $0.05. ✓
```

Reasoning in the output isn't just for show. It actually improves the model's computation.

<Expandable title="Why does chain-of-thought work?">

Two hypotheses:

**Computation through tokens**: The model does more computation when generating more tokens. Reasoning steps are where thinking happens. Skip them and you skip the thinking.

**Self-consistency**: Stating intermediate steps makes errors visible to the model. It can catch inconsistencies as it generates.

Whatever the cause, the effect is real. Complex tasks benefit from explicit reasoning, even when you don't need to see the steps yourself.

</Expandable>

**Structure your requests**

Clear structure helps the model parse your intent:

```
## Task
Summarize the following article.

## Requirements
- Maximum 3 paragraphs
- Include key statistics
- Maintain neutral tone

## Article
[paste article here]
```

Headers, bullet points, and explicit sections reduce ambiguity. The model knows exactly what's task, what's constraint, and what's input.

**Iterate and refine**

Prompt engineering is often iterative. Try something, see the result, adjust. Common refinements:

- Add constraints if output is too broad
- Remove constraints if output is too narrow
- Provide examples if format is wrong
- Ask for reasoning if accuracy is low
- Adjust length instructions if too long/short

There's rarely a perfect prompt on the first try. Treat prompting as a conversation, not a one-shot command.

<Metaphor title="Tuning a radio">

Prompting is like tuning an old radio dial. The signal (the model's capability) is there, but you need to find it. Small adjustments shift the output dramatically. Sometimes you're in static; a tiny turn brings clarity.

Prompt engineering is learning where the clear signals are and how to tune to them. Different tasks need different frequencies. Practice builds intuition for the dial.

</Metaphor>

<TryThis>

Take a prompt that gives mediocre results and try these modifications:
1. Add "Be concise" or "Be thorough"
2. Add an example of desired output
3. Add "Think step by step"
4. Add context about who you are and why you need this
Compare results. Notice which modifications help most for your specific task.

</TryThis>

**The limits of prompting**

Prompting can't make a model do what it fundamentally can't do. If the capability isn't in the model, no prompt will unlock it.

Prompting also can't guarantee consistency. Even with a perfect prompt, temperature adds variation. Different runs give different results.

Think of prompting as steering, not programming. You're guiding a capable system, not specifying exact behavior. Good prompts make desired outputs more likely, not certain.

<Sources>
<Citation type="docs" title="Prompt Engineering Guide" source="Anthropic" url="https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering" />
<Citation type="paper" title="Chain-of-Thought Prompting Elicits Reasoning in Large Language Models" authors="Wei et al." source="Google" url="https://arxiv.org/abs/2201.11903" year={2022} />
<Citation type="article" title="Prompt Engineering Guide" source="DAIR.AI" url="https://www.promptingguide.ai/" />
</Sources>
