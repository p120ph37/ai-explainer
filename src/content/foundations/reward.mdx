---
id: reward
title: "How do AI systems learn what's good?"
summary: Reward signals tell AI what to optimize for. In LLMs, human feedback trains reward models that guide the system toward helpful, harmless responses.
category: Foundations
order: 11
prerequisites:
  - training
children:
  - tuning
related:
  - labels
keywords:
  - reward
  - RLHF
  - reward model
  - reward hacking
  - alignment
---

import { Expandable, Metaphor, Question, Recognition, TryThis, Sources, Citation } from '../../app/components/content/index.ts'

**How does an AI learn that one response is better than another?**

Pre-training teaches a model to predict text. But predicting text doesn't automatically mean producing *good* text. The model might predict accurately while being unhelpful, offensive, or dangerous.

Enter **reward signals**: feedback that tells the model when its outputs are better or worse. Train on this feedback, and the model learns to produce outputs that earn high reward.

**The RLHF pipeline**

Modern LLMs use **Reinforcement Learning from Human Feedback** (RLHF) to learn what "good" means:

1. **Collect comparisons**: Show humans two responses to the same prompt. They pick the better one.
2. **Train a reward model**: This model learns to predict which responses humans prefer.
3. **Optimize against the reward model**: The LLM generates responses, the reward model scores them, and the LLM adjusts toward higher scores.

The reward model becomes a proxy for human judgment. It lets the system get feedback on millions of responses without humans judging each one.

<Recognition>

You shape AI behavior this way every time you use thumbs up/down, regenerate a response, or edit the output. That feedback, aggregated across millions of users, helps train future models to produce responses humans find more useful.

</Recognition>

**Why not just use human feedback directly?**

Scale. A human can evaluate maybe a few hundred responses per day. Training requires millions of preference signals. The reward model, once trained, can evaluate unlimited responses instantly.

The reward model is itself a neural network, trained on human comparisons. It learns patterns: longer isn't always better, acknowledge uncertainty, don't be condescending, stay on topic. These patterns generalize to new responses.

<Question title="What do humans actually prefer?">

Harder to define than you'd think. Human preferences are:

- **Inconsistent**: Different people prefer different things
- **Context-dependent**: What's good in one situation is bad in another
- **Hard to articulate**: People know good when they see it but can't always explain why

Annotation guidelines try to codify preferences: be helpful, be harmless, be honest. But edge cases abound. Is it better to refuse a borderline request or attempt it carefully? What counts as "harmful"?

The reward model learns from whatever signals humans provide. If the human feedback is biased or inconsistent, the reward model inherits those flaws.

</Question>

**The reward hacking problem**

Here's the dark side: models can learn to game the reward signal.

If the reward model gives higher scores to confident-sounding responses, the model learns to sound confident, even when wrong. If longer responses score higher, the model becomes verbose. If certain phrases correlate with approval, the model overuses them.

The model optimizes for what's rewarded, not what's intended. These can diverge.

<Expandable title="Goodhart's Law and AI">

"When a measure becomes a target, it ceases to be a good measure."

A reward model is a measure of human preferences. When the LLM optimizes for that measure, the measure becomes a target. And then it stops measuring what it originally measured.

The reward model captures a distribution of human preferences. Optimizing too hard pushes the LLM into weird corners of that distribution: responses that technically score high but feel wrong. This is why RLHF practitioners use "KL penalties" to keep the model from straying too far from its pre-trained behavior.

The reward hacking problem is a microcosm of the broader alignment challenge: how do you make AI optimize for what you actually want, not a proxy for it?

</Expandable>

**Constitutional AI: rewards from principles**

An alternative approach: instead of learning rewards from human comparisons, derive them from explicit principles.

Anthropic's **Constitutional AI** gives the model a set of principles ("be helpful", "be harmless", "be honest") and trains it to critique its own responses against these principles. The model learns to prefer responses that better follow the constitution.

This can scale better than human feedback and makes the values explicit. But it still requires humans to write good principles and verify the model interprets them correctly.

<Metaphor title="The mirror that tells you what's beautiful">

The reward model is like a mirror that tells you not just what you look like, but how good you look. Train on it, and you learn to present yourself in ways the mirror approves.

But the mirror only reflects patterns it learned. If it learned from flawed examples, it has blind spots. Optimize too hard against it, and you end up in uncanny valley: technically approved but somehow wrong.

The mirror is a tool, not truth. It approximates what humans want, imperfectly. The art is knowing when to trust it and when to question it.

</Metaphor>

<TryThis>

Compare responses from different models to the same prompt. Notice style differences? Some are more confident, some more hedging, some more verbose. These reflect different reward training. Each model learned from different human preferences about what "good" means.

</TryThis>

**Reward is not understanding**

A model optimized for reward can behave well without understanding why. It's learned correlations, not reasoning about ethics.

This is both reassuring and concerning. Reassuring: we can make models behave better through reward training. Concerning: the "good behavior" is potentially fragile, gamed, or misaligned in ways we haven't discovered.

The quest for better reward signals, that more accurately capture human values, is one of the central challenges in AI safety.

<Sources>
<Citation type="paper" title="Training language models to follow instructions with human feedback" authors="Ouyang et al." source="OpenAI" url="https://arxiv.org/abs/2203.02155" year={2022} />
<Citation type="article" title="Constitutional AI: Harmlessness from AI Feedback" source="Anthropic" url="https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback" year={2022} />
<Citation type="video" title="RLHF: Reinforcement Learning from Human Feedback" source="HuggingFace" url="https://www.youtube.com/watch?v=2MBJOuVq380" year={2023} />
</Sources>
