---
id: memory
title: "How do LLMs remember conversations?"
summary: LLMs are stateless. Each request starts fresh. What feels like memory is the conversation history being re-sent every time. Real memory requires external systems.
category: Foundations
order: 24
prerequisites:
  - intro
  - context-window
  - tokens
children: []
related:
  - context-window
  - applications
  - vector-databases
keywords:
  - memory
  - conversation history
  - context
  - stateless
  - chat history
---

import { Expandable, Metaphor, Question, Recognition, TryThis, Sources, Citation } from '../../app/components/content/index.ts'
import { FlowDiagram } from '../../app/components/diagrams/index.ts'

**How does the AI remember what we discussed earlier?**

It doesn't. Not really.

LLMs are **stateless**. Each API call is independent. The model doesn't retain information between requests. It doesn't remember you, your previous questions, or what it said before.

What feels like memory is an illusion created by resending the entire conversation history with each new message.

**The context window trick**

When you chat with an AI:

```
Message 1: "My name is Alex"
Response 1: "Nice to meet you, Alex!"

Message 2: "What's my name?"
```

Behind the scenes, message 2 actually sends:

```
User: My name is Alex
Assistant: Nice to meet you, Alex!
User: What's my name?
```

The model sees the full conversation and "remembers" by reading its own previous responses. This isn't memory. It's reading a transcript.

<Recognition>

You've seen the limits of this. Long conversations eventually lose coherence. The AI forgets things from earlier. Details you mentioned get contradicted. That's the context window filling up and old messages being dropped.

</Recognition>

**Why statelessness?**

Stateless design has benefits:

- **Scalability**: Any server can handle any request; no session affinity needed
- **Reliability**: No state to lose if a server fails
- **Simplicity**: Each request is self-contained
- **Privacy**: No persistent storage of conversations (by default)

But it means "memory" must be explicitly managed by the application layer.

<Question title="What happens when the context fills up?">

Context windows have limits (4K, 32K, 128K+ tokens). When conversation history exceeds this:

- **Truncation**: Old messages are dropped (first-in, first-out)
- **Summarization**: Old content is summarized to save tokens
- **Sliding window**: Keep recent context, drop old context

Each approach loses information. The model literally forgets. Those tokens are gone.

Long conversations eventually hit this wall. The AI contradicts itself, forgets your name, or asks questions you already answered. It's not careless; it can no longer see that information.

</Question>

**Beyond context: external memory systems**

True memory requires storage outside the model:

**Vector databases**: Store conversation snippets as embeddings. Retrieve relevant past interactions when needed. "Remember when we discussed X?" triggers retrieval.

**Key-value stores**: Store specific facts. "User prefers dark mode" persists across sessions without consuming context.

**Conversation summaries**: Periodically summarize conversations. Store summaries for later retrieval. Compress history without losing key points.

**Hybrid approaches**: Keep recent messages in full context, retrieve from long-term storage for older interactions.

<FlowDiagram
  title="Memory Architecture"
  steps={[
    { id: '1', label: 'New Message', sublabel: 'User input', icon: 'ðŸ’¬' },
    { id: '2', label: 'Retrieve', sublabel: 'Relevant memories', icon: 'ðŸ”' },
    { id: '3', label: 'Construct', sublabel: 'Context + memories', icon: 'ðŸ“‹' },
    { id: '4', label: 'Generate', sublabel: 'LLM response', icon: 'ðŸ§ ' },
    { id: '5', label: 'Store', sublabel: 'Update memories', icon: 'ðŸ’¾' },
  ]}
  direction="horizontal"
  ariaLabel="Flow diagram showing memory retrieval and storage cycle"
/>

<Expandable title="Memory in practice">

Different applications handle memory differently:

**ChatGPT / Claude**: Store conversation threads. Each thread has full history up to context limits. New threads start fresh.

**Personal AI assistants**: May store user preferences, past interactions, and personal facts in external databases.

**Customer service bots**: Often retrieve customer history from CRM systems. The model doesn't remember; it's told about the customer each time.

**Coding assistants**: May index your codebase. "Remember" your code by retrieving relevant files, not by persistent memory.

The model itself never remembers. The application manages what to tell it.

</Expandable>

<Metaphor title="Reading a script">

The LLM is like an actor who reads a script fresh each scene. They don't remember filming yesterday. They read the script notes describing what happened.

If the script notes are incomplete, they might contradict previous scenes. If the notes are detailed, they perform consistently. The "memory" is entirely in the script, not in the actor's head.

Each API call hands the model a new script. Good applications write good scripts. The model performs what it reads.

</Metaphor>

**Working memory vs. long-term memory**

Think of it in two layers:

**Working memory** (context window):
- Immediate, limited capacity
- Exactly what's in the current prompt
- Fast but ephemeral
- Lost when context is cleared

**Long-term memory** (external systems):
- Persistent across sessions
- Requires explicit retrieval
- Can be structured or semantic
- Application-dependent

Neither is truly "in" the model. Working memory is the prompt. Long-term memory is databases the application queries.

<TryThis>

Start a new conversation with an AI. Tell it a specific, unusual fact: "My favorite number is 7,432." Have a long conversation about other topics. Eventually ask "What's my favorite number?" See if it remembers. Try again in a new conversation. It won't know. That's the difference between context (temporary) and memory (persistent).

</TryThis>

**The memory illusion**

When AI feels like it remembers you:
- An application stored your preferences
- Your conversation history was retrieved and included
- Previous summaries were injected into the prompt
- An external database was queried

The model is told these things. It doesn't recall them. The magic is in the application layer, not the model.

Understanding this helps you work with AI more effectively. Want it to "remember" something important? Make sure the application stores it. Long conversation losing coherence? Start fresh with a summary of key points.

<Sources>
<Citation type="article" title="Memory and Context" source="LangChain" url="https://python.langchain.com/docs/modules/memory/" />
<Citation type="docs" title="Multi-turn Conversations" source="Anthropic" url="https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching" />
</Sources>
