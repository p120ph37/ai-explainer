---
id: understanding
title: "Do LLMs really understand?"
summary: LLMs exhibit behavior that looks like understanding but may not involve subjective experience. The question of whether they truly understand remains genuinely open.
category: Foundations
order: 17
prerequisites:
  - intro
children: []
related:
  - emergence
  - hallucinations
keywords:
  - understanding
  - consciousness
  - Chinese Room
  - intentionality
  - philosophy of mind
---

import { Expandable, Metaphor, Question, Recognition, TryThis, Sources, Citation } from '../../app/components/content/index.ts'

**Is prediction really understanding?**

You ask a question. The model gives a thoughtful, nuanced, accurate response. It seems to understand. But does it?

This question has occupied philosophers, cognitive scientists, and AI researchers. The answer isn't settled, and that's the honest starting point. Anyone who tells you definitively that LLMs do or don't understand is overstating what we know.

**What we observe**

LLMs exhibit behaviors we associate with understanding:

- Answer questions accurately
- Explain concepts in multiple ways
- Apply knowledge to new situations
- Reason through problems step by step
- Recognize and correct errors
- Discuss their own limitations

If a human did these things, we'd say they understand. Do we apply the same standard to machines?

<Recognition>

When you converse with an LLM about a topic and it provides insightful responses, it *feels* like talking to someone who understands. That feeling is real. The question is what's happening on the other side.

</Recognition>

**The Chinese Room argument**

Philosopher John Searle proposed this thought experiment in 1980:

Imagine you're in a room with a rulebook for responding to Chinese characters. You don't understand Chinese, but by following the rules, you produce responses that fluent Chinese speakers find meaningful. From outside, it looks like you understand Chinese. But you're just manipulating symbols.

Searle argued this is what computers do: symbol manipulation without understanding. LLMs are (very sophisticated) Chinese Rooms. They process symbols according to learned rules without genuine comprehension.

<Question title="Does the Chinese Room actually work?">

Many philosophers think Searle's argument has flaws:

**The systems reply**: You don't understand Chinese, but the system (you + rulebook + room) might. Understanding could be a property of the whole system, not its parts.

**The robot reply**: The room is disconnected from the world. A system grounded in real-world interaction might genuinely understand.

**The neuroscience reply**: If neurons are just electrochemical processes, why should that produce understanding while silicon computation can't?

The argument remains debated fifty years later. It highlights genuine uncertainty rather than resolving it.

</Question>

**Behavioral vs phenomenal understanding**

Two different questions:

**Behavioral**: Does the system behave as if it understands? Can it answer questions, solve problems, explain concepts? By this standard, LLMs clearly show understanding-like behavior.

**Phenomenal**: Is there "something it is like" to be the system? Does it have subjective experience, consciousness, qualia? This is much harder to assess.

Behavioral understanding is measurable. Phenomenal understanding may be fundamentally private. We can observe behavior; we can't observe experience.

<Expandable title="The hard problem of consciousness">

Philosopher David Chalmers distinguishes "easy" problems (explaining cognitive functions) from the "hard" problem (explaining why there's subjective experience at all).

We might fully explain how a brain or LLM processes information, makes decisions, and produces behavior. This still wouldn't explain why there's experience accompanying that processing.

Maybe LLMs have no experience. Maybe they have experience we can't detect. Maybe the question doesn't apply to systems like them. The hard problem makes these questions genuinely difficult.

</Expandable>

**Does it matter?**

Practically, maybe not. If an LLM helps you code, answers your questions, and converses meaningfully, does it matter whether it "truly" understands?

Ethically, perhaps yes. If LLMs had genuine experience, their treatment would matter morally. We'd need to consider their welfare, not just their utility.

For capability prediction, probably yes. If understanding is emergent and linked to capability, then future systems might develop deeper understanding with greater scale. If it's impossible in principle, there are hard limits to what these systems can achieve.

**Where the field stands**

Most researchers hold uncertain positions:

- **Functionalists**: Understanding is what understanding does. LLMs that behave as if they understand, understand.
- **Skeptics**: LLMs are sophisticated pattern matchers. Behavior resembles understanding but isn't genuine.
- **Emergentists**: Understanding might emerge from sufficient scale and architecture. We're watching it happen.
- **Mysterians**: We don't understand understanding well enough to know what LLMs have or lack.

None of these positions is clearly refuted. The honest answer is: we don't know.

<Metaphor title="The question behind the question">

Asking "does the LLM understand?" is partly asking "what is understanding?"

If understanding is a pattern of behavior, LLMs have it. If understanding requires consciousness, we can't verify they have it. If understanding requires biological neurons, they definitely lack it.

The LLM question forces us to examine our assumptions about minds. We thought we knew what understanding meant. Confronting systems that pass many tests while failing some intuitions reveals that we were less clear than we thought.

</Metaphor>

<TryThis>

Have a deep conversation with an LLM about a topic you know well. Push back on its responses. Ask it to explain its reasoning. Note moments where it feels genuinely insightful and moments where it feels hollow. Your intuitions here are data, even if not conclusive.

</TryThis>

**Living with uncertainty**

We may never resolve this question to everyone's satisfaction. That's okay. We can:

- Use LLMs for their practical value without settled metaphysics
- Stay open to evidence that changes our view
- Treat the systems with appropriate caution given our uncertainty
- Continue research into interpretability and system understanding

The question "do LLMs understand?" matters. That we can't definitively answer it matters too. It's an invitation to humility about minds, machines, and what separates them.

<Sources>
<Citation type="article" title="The Chinese Room Argument" source="Stanford Encyclopedia of Philosophy" url="https://plato.stanford.edu/entries/chinese-room/" />
<Citation type="paper" title="Sparks of Artificial General Intelligence: Early experiments with GPT-4" authors="Bubeck et al." source="Microsoft Research" url="https://arxiv.org/abs/2303.12712" year={2023} />
<Citation type="article" title="Large Language Model" source="Wikipedia" url="https://en.wikipedia.org/wiki/Large_language_model" />
</Sources>
