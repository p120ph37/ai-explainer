---
id: emergence
title: "What is emergent behavior?"
summary: Emergence is when simple rules produce complex behavior. In LLMs, capabilities like reasoning appear spontaneously at scale, not from explicit programming.
prerequisites:
  - why-large
children: []
related:
  - training
  - neural-network
keywords:
  - emergence
  - emergent capabilities
  - phase transitions
  - complexity
---

import { Expandable, Metaphor, Question, Recognition, TryThis, Sources, Citation } from '../../app/components/content/index.ts'

**Why do capabilities suddenly appear at certain scales?**

You might expect AI capabilities to improve smoothly as models get bigger. Instead, some abilities are absent in smaller models, then suddenly present in larger ones. Like flipping a switch.

This is **emergence**: capabilities that arise from scale without being explicitly programmed. The model wasn't trained to do arithmetic. Yet at sufficient size, it can. Nobody taught it to translate between languages it rarely saw paired. Yet it does.

**Emergence beyond AI**

This phenomenon isn't unique to language models. It appears throughout nature and mathematics:

- **Water**: Individual H₂O molecules don't have wetness. But enough molecules together are wet.
- **Flocking**: Each bird follows simple rules. The flock exhibits complex, coordinated patterns.
- **Consciousness**: Individual neurons aren't conscious. Yet somehow, enough neurons together produce experience.
- **Cities**: No central planner designs traffic patterns. They emerge from individual decisions.

In each case, the whole exhibits properties absent from the parts. Simple components, simple rules, complex outcomes.

<Recognition>

You've seen emergence in games. Conway's Game of Life has four simple rules about cells turning on and off. From these rules emerge gliders, oscillators, and patterns that compute. The rules never mention gliders. They emerge from the dynamics.

</Recognition>

**Emergent capabilities in LLMs**

Documented emergent capabilities include:

- **Multi-step arithmetic**: Small models can't add three-digit numbers. Large models can.
- **Chain-of-thought reasoning**: The ability to work through problems step by step.
- **Cross-lingual transfer**: Learning a skill in one language, applying it in another.
- **Theory of mind**: Modeling what other agents might believe or want.
- **Code execution**: Mentally tracing through code to predict outputs.

These capabilities appear at different scale thresholds. Below the threshold: failure. Above it: success. The transition can be sharp.

<Question title="Is emergence real or a measurement artifact?">

This is debated. Some researchers argue emergent capabilities are artifacts of how we measure. If we used different metrics, we might see gradual improvement instead of sudden jumps.

Others point to genuine phase transitions: qualitative changes in what the model can represent. Like water freezing, there may be critical points where system behavior fundamentally shifts.

The debate continues. What's undeniable is that *something* happens at scale that we didn't predict and can't easily explain. Whether we call it emergence or something else, it's surprising.

</Question>

**The primordial soup connection**

There's a deep parallel to the origin of life.

Simple molecules, energy sources, and time. From these emerged self-replicating structures, metabolism, eventually cells and creatures. Nobody programmed DNA. It emerged from chemistry under consistent selective pressure.

LLM training provides analogous conditions: simple components (parameters), energy (compute), consistent pressure (prediction loss). Under these conditions, complex capabilities crystallize.

This isn't proof that LLMs are alive or conscious. But it suggests that complex behavior emerging from simple rules under selection is a general phenomenon, not magic.

<Expandable title="Scaling laws and phase transitions">

Physicists study phase transitions: points where system behavior changes qualitatively (water → ice, magnet → non-magnet). These transitions often follow power laws near critical points.

LLM scaling laws show similar power-law behavior. Loss decreases predictably with scale. But capabilities don't always follow loss smoothly. Some capabilities might be "phase transitions" in the model's representational structure.

If true, this would mean some capabilities are fundamentally impossible below certain scales. Not because we haven't found the right training, but because the model lacks sufficient capacity for the required representations.

</Expandable>

**Why does prediction create reasoning?**

This is the core mystery. The training objective is simple: predict the next token. How does this create capabilities like reasoning or planning?

One hypothesis: to predict well across diverse text, you must model the processes that generate text. Humans reason, plan, and know facts. Text reflects this. To predict text well, the model must develop something like reasoning, planning, and factual knowledge.

The prediction task is simple. But *achieving* good prediction on all human text is not simple. It requires modeling the full complexity of human thought.

<Metaphor title="A game that teaches everything">

Picture a game where you predict what someone will say next, across all conversations ever. To win, you must model:
- Grammar (to predict syntactically valid continuations)
- Facts (to predict accurate statements)
- Logic (to predict valid arguments)
- Emotions (to predict responses in dialogues)
- Intent (to predict where the conversation goes)

This one game, played at sufficient scale and difficulty, teaches everything needed to generate human-like text. Not because the game is complex, but because human text is complex, and predicting it requires understanding the full space of human expression.

</Metaphor>

<TryThis>

Try Conway's Game of Life (available free online). Watch patterns evolve from random starting points. See gliders emerge. Notice how complexity arises without being designed. This is emergence in action, made visible. LLM emergence is similar but harder to see: complex capabilities emerging from simple gradient updates.

</TryThis>

**Emergence and unpredictability**

Emergence is partly why AI capabilities are hard to forecast. We can predict loss improvements from scaling laws. We cannot easily predict *which capabilities* will emerge at which scales.

This makes frontier AI development partly exploratory. You build bigger models partly to discover what they can do. The capabilities weren't specified in advance; they're found empirically.

This is both exciting and concerning. Exciting because new capabilities await discovery. Concerning because we can't anticipate what capabilities might appear, including potentially dangerous ones.

<Sources>
<Citation type="paper" title="Emergent Abilities of Large Language Models" authors="Wei et al." source="Google Research" url="https://arxiv.org/abs/2206.07682" year={2022} />
<Citation type="paper" title="Are Emergent Abilities of Large Language Models a Mirage?" authors="Schaeffer et al." source="Stanford" url="https://arxiv.org/abs/2304.15004" year={2023} />
<Citation type="article" title="Emergence" source="Wikipedia" url="https://en.wikipedia.org/wiki/Emergence" />
</Sources>
