---
id: neural-network
title: "What is a neural network?"
summary: Neural networks are computing systems loosely inspired by biological brains. They learn patterns from data by adjusting millions of numerical connections.
category: Foundations
order: 4
prerequisites:
  - intro
children:
  - parameters
  - attention
related:
  - training
  - embeddings
keywords:
  - neural network
  - perceptron
  - layers
  - neurons
  - weights
---

import { Expandable, Metaphor, Question, Recognition, TryThis, Sources, Citation } from '../../app/components/content/index.ts'
import { LayerStack, PerceptronToy } from '../../app/components/diagrams/index.ts'

**What's actually inside an AI model?**

A neural network is a mathematical function built from simple, repeated building blocks. Each block takes some numbers in, does basic math, and passes numbers out. Stack thousands of these blocks in careful arrangements, and something remarkable happens: the system can learn.

The "neural" part comes from a loose analogy to brain neurons. But don't take it too literally. These are mathematical operations, not biological cells.

<Question title="How similar are neural networks to actual brains?">

Very loosely similar. Both have units that combine weighted inputs and produce outputs. Both learn by adjusting connection strengths. But real neurons are vastly more complex: they spike, they're chemical, they have temporal dynamics.

Neural networks are simplified mathematical abstractions, not brain simulations. The name is more marketing than neuroscience. That said, some researchers argue the loose similarity may be enough: at sufficient scale, even simplified approximations might produce similar emergent capabilities.

</Question>

**The simplest case: the perceptron**

To understand neural networks, start with the simplest possible one: a single unit called a **perceptron**.

A perceptron takes multiple inputs (numbers), multiplies each by a **weight** (another number), adds them up, and outputs a result. That's it. Multiply, add, output.

```
inputs:  [x₁, x₂, x₃]
weights: [w₁, w₂, w₃]
output:  w₁·x₁ + w₂·x₂ + w₃·x₃ + bias
```

<PerceptronToy title="Interactive Perceptron" />

The magic is in the weights. By adjusting them, the perceptron can learn to make different decisions. High weight on an input means "pay attention to this." Low or negative weight means "ignore or invert this."

<Recognition>

**You do something like this intuitively.** When deciding whether to go outside, you might weight "is it raining?" heavily negative, weight "do I have errands?" heavily positive, and weight "what's on TV?" slightly negative. Different weights, different decisions.

You also encounter neural networks more often than you realize. Face unlock on your phone, camera filters that track your face, voice assistants responding to your speech. These all run neural networks in real-time.

</Recognition>

<Question title="If it's just 'multiply, add, output', how does that become intelligence?">

Composition and scale. One multiply-add is trivial. Billions of them, stacked in layers, with each layer transforming what the previous layer outputs, can approximate any function.

The universal approximation theorem says sufficiently large networks can represent arbitrarily complex input-output mappings. Simple operations composed at scale produce complex behavior.

→ [Why Does Scale Matter?](/scale)

</Question>

**From one to many: layers**

A single perceptron can only learn simple patterns (technically, linear separations). But stack them into **layers**, where the outputs of one layer become the inputs to the next, and the network can learn complex patterns.

- **Input layer**: Your raw data (pixels of an image, tokens of text)
- **Hidden layers**: Middle layers that transform and combine features
- **Output layer**: The final answer (a classification, a probability distribution over next tokens)

<LayerStack
  title="Neural Network Layers"
  layers={[
    { id: 'input', label: 'Input Layer', sublabel: 'Raw data (tokens, pixels)', color: 'var(--color-surface)' },
    { id: 'hidden1', label: 'Hidden Layer 1', sublabel: 'Low-level features' },
    { id: 'hidden2', label: 'Hidden Layer 2', sublabel: 'Combinations of features' },
    { id: 'hidden3', label: 'Hidden Layer ...', sublabel: 'Higher abstractions' },
    { id: 'output', label: 'Output Layer', sublabel: 'Final prediction', color: 'var(--color-surface)' },
  ]}
  direction="up"
  ariaLabel="Diagram showing neural network layers from input at bottom to output at top"
/>

Each layer extracts more abstract features. In an image network, early layers might detect edges, middle layers might detect shapes, and later layers might detect faces. Nobody programs these features; they emerge from training.

<Metaphor title="The layer cake">

The network isn't flat. It's layered, like a cake with many tiers.

Signals enter at the bottom tier (your input). They flow upward through hidden tiers, transforming at each level. By the time they reach the top tier (the output), they've been mixed, combined, filtered, and reshaped countless times.

Depth allows abstraction to build on abstraction. The twentieth layer can work with concepts that the first layer has no vocabulary for, because the intervening layers built that vocabulary through successive transformation. Shallow networks are limited to shallow transformations. Depth is how complexity becomes possible.

</Metaphor>

<Question title="Why are they called 'hidden' layers?">

Because you don't directly observe them. You see what goes in (inputs) and what comes out (outputs), but the intermediate computations are internal to the network. They're hidden from the outside view.

The term isn't profound. It just means "not input, not output, somewhere in between."

</Question>

<Question title="What determines how many layers a network should have?">

Empirical tuning. Deeper networks can represent more abstract features but are harder to train (vanishing gradients). Architecture innovations (residual connections, layer normalization) enabled training very deep networks.

Modern LLMs have 50-100+ layers. The optimal depth depends on task, data, and compute budget. There's no formula. Researchers experiment.

→ [What is a Transformer?](/transformer)

</Question>

**What makes them learn?**

A neural network starts with random weights. It makes terrible predictions. Then training begins:

1. Show the network an example
2. Compare its output to the correct answer
3. Calculate how wrong it was (the "loss")
4. Adjust the weights slightly to be less wrong
5. Repeat millions of times

This process is called **gradient descent**. "Gradient" refers to the mathematical slope that tells you which direction to adjust each weight. "Descent" because you're descending toward lower error.

<Metaphor title="The gradient river">

Training follows a river downstream.

The loss function is altitude: how wrong the output was. Every combination of dial settings defines a point in a vast landscape. Training is finding low points: settings where outputs are correct.

Gradient descent is feeling which way is downhill and taking a step. The gradient is the slope. Step after step, the system descends toward valleys where error is low. The landscape has billions of dimensions, one per weight. You can't visualize it. But mathematically, downhill still exists, and the algorithm follows it.

</Metaphor>

<Question title="How does the network know which weights to adjust?">

**Backpropagation**, powered by calculus. The chain rule lets you compute, for each weight, "how much would the error change if I nudged this weight?" This is the gradient. Every weight gets a gradient. Then every weight is nudged proportionally.

Starting from the output, it propagates the error signal backward through the network, computing gradients layer by layer. The math involves calculus (chain rule), but the intuition is simple: trace the blame backward. If the output was wrong, which weights made it wrong? And which weights before that?

→ [How are LLMs Trained?](/training)

</Question>

<Question title="What's 'vanishing gradients' and why does it matter?">

Gradients are the mechanism by which training feedback adjusts the model. If gradients shrink at each layer (multiply by numbers less than 1), they become negligibly small by the time they reach early layers. Early layers stop learning.

This is the "vanishing gradient problem." Solutions include careful weight initialization, normalization layers, and residual connections that let gradients skip layers.

</Question>

**How big are these networks?**

Size varies enormously:

- A perceptron: 10-100 weights
- A simple image classifier: millions of weights
- GPT-3: 175 billion weights
- Frontier models (GPT-5.1): 1-2+ trillion weights

Each weight is a number, typically stored as 16 or 32 bits. GPT-3's weights alone take about 350 gigabytes to store. Running the network requires loading these weights and performing matrix multiplications across them.

<Metaphor title="A vast switchboard">

Picture an old telephone switchboard with billions of connections. Each connection has a dial controlling its strength. Some connections amplify signals, others dampen them, others invert them.

When you input a message, signals flow through this switchboard. At each junction, signals are combined and transformed based on the dial settings. The final output emerges from this cascade of combinations.

No single dial "knows" anything. Ask which dial stores "Paris is the capital of France" and there's no answer. That knowledge is distributed across millions of dials whose combined effect produces correct outputs for Paris-related queries. It's like asking which water molecule contains wetness. Wetness is emergent, a property of the whole, not localized in any part.

Training is an operator adjusting billions of dials, one tiny click at a time, until the switchboard routes messages to correct outputs.

</Metaphor>

<Question title="Why not more precision? 64-bit weights?">

Diminishing returns and memory costs. Higher precision means larger files and slower computation. Research shows 16-bit (often 8-bit or even 4-bit) works nearly as well for many applications.

The network has redundancy. Small precision losses average out across billions of parameters. Quantization sacrifices precision for efficiency, often with minimal quality loss.

→ [What are Parameters?](/parameters)

</Question>

<TryThis>

Many "neural network playground" sites let you visualize small networks learning in real-time. Try [TensorFlow Playground](https://playground.tensorflow.org/). Watch how the decision boundary evolves as the network trains. Add more layers and see how it can learn more complex patterns.

</TryThis>

**Why does this work at all?**

Neural networks exploit a mathematical property: sufficiently large networks can approximate any function. This is the "universal approximation theorem." Give a network enough units and it can, in principle, learn any input-output mapping.

But "can in principle" doesn't mean "will in practice." The genius is in architectures (how you arrange the layers), training procedures (how you adjust weights), and data (what examples you show). These determine whether a network actually learns something useful.

<Question title="What's stopping us from making infinitely large networks?">

Physical limits. Memory to store parameters, compute to process them, energy to power the hardware, time to train. Current frontier models push available resources.

Larger models exist but are more expensive to train and run. Scaling laws show diminishing returns eventually. We may hit limits where bigger isn't better, or where costs become prohibitive.

→ [Why Does Scale Matter?](/scale)

</Question>

Neural networks are not magic. They're math. But math that, stacked deep enough and trained on enough data, produces capabilities that continually surprise us.

<Sources>
<Citation type="video" title="But what is a neural network?" source="3Blue1Brown" url="https://www.youtube.com/watch?v=aircAruvnKk" year={2017} />
<Citation type="article" title="Neural network" source="Wikipedia" url="https://en.wikipedia.org/wiki/Artificial_neural_network" />
<Citation type="docs" title="TensorFlow Playground" source="TensorFlow" url="https://playground.tensorflow.org/" />
</Sources>
