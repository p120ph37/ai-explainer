---
id: reasoning
title: "How do LLMs reason?"
summary: "LLMs reason better when they \"think out loud.\" Chain-of-thought prompting and reasoning models show that more compute at inference time improves complex problem-solving."
prerequisites:
  - intro
  - inference
  - prompt-engineering
keywords:
  - reasoning
  - chain of thought
  - CoT
  - o1
  - thinking
  - step by step
---

import { Expandable, Metaphor, Question, Recognition, TryThis, Sources, Citation } from '@/app/components/content/index.ts'

**Why does "think step by step" make AI smarter?**

Ask an LLM a complex question directly, and it might fail. Ask it to explain its reasoning step by step, and it often succeeds. The same model, the same knowledge, dramatically different results.

This is **chain-of-thought (CoT)** prompting: getting models to show their work. It's not a trick. It reflects something fundamental about how these models reason.

**The problem with direct answers**

LLMs generate token by token, left to right. Each token is based on what came before. When forced to answer immediately, the model must compress all reasoning into choosing the first token of its answer.

```
Q: If a train travels 60 mph for 2.5 hours, how far does it go?
A: 150 miles

The model had to "know" 150 immediately, with no scratch work.
```

For simple questions, this works. For complex reasoning, it's like solving calculus in your head without writing anything down. Possible sometimes; unreliable often.

**Chain of thought: external working memory**

When the model "thinks out loud," its own generated text becomes working memory:

```
Q: If a train travels 60 mph for 2.5 hours, how far does it go?

A: Let me work through this step by step.
- Speed: 60 miles per hour
- Time: 2.5 hours  
- Distance = Speed × Time
- Distance = 60 × 2.5
- 60 × 2 = 120
- 60 × 0.5 = 30
- 120 + 30 = 150
The train travels 150 miles.
```

Each step becomes tokens that the model can attend to when generating the next step. The reasoning is externalized, visible, checkable.

<Recognition title="Let's think step by step">

You've seen this if you've added "Let's think step by step" or "Explain your reasoning" to prompts and gotten better results. You're triggering chain-of-thought reasoning, giving the model permission and space to work through the problem.

</Recognition>

**Why does this work?**

Several factors:

**Serial computation**: Complex problems require multiple reasoning steps. CoT provides the "space" for those steps to happen sequentially.

**Error correction**: With visible intermediate steps, the model can notice and correct mistakes. Direct answers have no self-check opportunity.

**Pattern activation**: The format of step-by-step reasoning resembles training data showing problem-solving. It triggers helpful patterns.

**Decomposition**: Breaking problems into subproblems makes each step easier. "What's 60 × 2.5?" is harder than "What's 60 × 2? What's 60 × 0.5? What's their sum?"

<Question title="Does the model actually 'think' more?">

Not in the hidden layers. The neural network computation per token remains similar. What changes is the number of tokens generated, and thus the total computation.

A 10-word direct answer uses ~10 forward passes. A 100-word chain-of-thought uses ~100 forward passes. More tokens, more total compute, more opportunity for the model to process the problem.

CoT trades tokens (and time and cost) for accuracy. The model isn't thinking harder per step; it's taking more steps.

</Question>

**Reasoning models: thinking deeply**

Recent models (like OpenAI's o1) take this further. Instead of just chain-of-thought in the visible output, they do extensive internal reasoning before responding.

The model might:
- Explore multiple approaches
- Verify its own conclusions
- Reconsider and revise
- Spend many more tokens thinking than answering

This "test-time compute" (computation spent during inference rather than training) significantly improves performance on hard problems.

```
Standard model: Think briefly → Answer
Reasoning model: Think at length → Verify → Revise → Think more → Answer
```

<Expandable title="The scaling of inference">

Traditional scaling focused on training: bigger models, more data, more compute during training.

Reasoning models reveal another scaling axis: inference compute. Spending more time "thinking" on each problem improves results.

This has implications:
- Hard problems can warrant more inference compute
- Users might pay for "thinking time" not just tokens
- The same model can have different capability levels based on inference budget

Scaling training made models more capable. Scaling inference makes them more reliable on the capabilities they have.

</Expandable>

**Techniques for better reasoning**

Several approaches encourage reasoning:

**Zero-shot CoT**: Just add "Let's think step by step" to the prompt.

**Few-shot CoT**: Show examples of step-by-step reasoning before the question.

**Self-consistency**: Generate multiple reasoning chains, take the majority answer.

**Tree of thought**: Explore multiple reasoning paths, backtrack when stuck.

**Verification**: Ask the model to check its own answer; let it revise.

**Decomposition**: Break complex questions into simpler sub-questions.

<Metaphor title="Showing your work">

Remember being told to "show your work" in math class? Not just for partial credit. Working through steps catches errors and structures thinking.

An LLM forced to give immediate answers is like a student writing only final answers. Sometimes right, often wrong, hard to debug. An LLM showing its work is like a student writing out each step. Easier to verify, easier to catch mistakes, easier to learn from.

The model's "paper" is its output tokens. Showing work means generating those tokens.

</Metaphor>

**Limits of reasoning**

Chain-of-thought isn't magic:

- **Token cost**: Extensive reasoning uses many tokens, increasing cost and latency
- **Faithfulness**: Stated reasoning may not reflect actual "thinking"—models can confabulate explanations
- **Ceiling**: More thinking time helps, but doesn't grant new capabilities
- **Error propagation**: Mistakes in early steps compound
- **Problem fit**: Works best for problems with clear step structure; less helpful for intuitive leaps

Reasoning improves reliability, not raw capability. A model that can't solve a problem won't solve it by thinking longer, but a model that sometimes can will succeed more often.

<TryThis>

Take a word problem or logic puzzle. First ask an LLM for the direct answer. Then ask again, starting with "Let's work through this step by step. First..." Compare both accuracy and your ability to verify the answer. The chain-of-thought version may be longer, but it's easier to trust or debug.

</TryThis>

**The future of reasoning**

Reasoning capability is a frontier. Current directions:

- **Longer thinking**: Models that can "ponder" for minutes, not seconds
- **Learned reasoning**: Training models to develop their own reasoning strategies
- **Tool-augmented reasoning**: Using code execution and search within reasoning chains
- **Verifiable reasoning**: Formal methods to check reasoning validity

The insight that inference-time computation matters is reshaping how we think about AI capability. Training determines potential; inference determines realization.

<Sources>
<Citation type="paper" title="Chain-of-Thought Prompting Elicits Reasoning in Large Language Models" authors="Wei et al." source="Google" url="https://arxiv.org/abs/2201.11903" year={2022} />
<Citation type="paper" title="Self-Consistency Improves Chain of Thought Reasoning in Language Models" authors="Wang et al." source="Google" url="https://arxiv.org/abs/2203.11171" year={2022} />
<Citation type="article" title="Extended Thinking" source="Anthropic" url="https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking" />
</Sources>
