---
id: applications
title: "What makes an AI application?"
summary: "LLMs are engines, not products. Applications wrap them with retrieval, tools, guardrails, system prompts, and agentic loops to create useful, safe products."
category: Ecosystem
order: 4
prerequisites:
  - intro
  - prompt-engineering
  - tools
children: []
related:
  - vector-databases
  - guardrails
  - agents
  - tools
  - security
keywords:
  - AI applications
  - agents
  - system prompts
  - guardrails
  - orchestration
  - RAG
  - retrieval
---

import { Expandable, Metaphor, Question, Recognition, TryThis, Sources, Citation } from '../../app/components/content/index.ts'
import { FlowDiagram } from '../../app/components/diagrams/index.ts'

**Why does the same LLM feel so different in different products?**

Claude powers both a friendly chatbot and a code editor. GPT-4 runs everything from customer service bots to research assistants. The base model is the same, but the experiences feel different.

The difference is the **application layer**: everything wrapped around the raw LLM to make it suitable for a specific task. LLMs are engines; applications are vehicles.

**The layers of an AI application**

A raw LLM API gives you completion: send text, get text back. To build a product, you add layers:

<FlowDiagram
  title="AI Application Stack"
  steps={[
    { id: '1', label: 'User Input', sublabel: 'Question or request', icon: 'ðŸ‘¤' },
    { id: '2', label: 'Guardrails', sublabel: 'Input filtering', icon: 'ðŸ›¡ï¸' },
    { id: '3', label: 'System Prompt', sublabel: 'Persona & rules', icon: 'ðŸ“‹' },
    { id: '4', label: 'Retrieval', sublabel: 'RAG context', icon: 'ðŸ“š' },
    { id: '5', label: 'LLM', sublabel: 'Core reasoning', icon: 'ðŸ§ ' },
    { id: '6', label: 'Tool Loop', sublabel: 'Actions if needed', icon: 'ðŸ”§' },
    { id: '7', label: 'Output Guards', sublabel: 'Response filtering', icon: 'ðŸ›¡ï¸' },
    { id: '8', label: 'Response', sublabel: 'To user', icon: 'ðŸ’¬' },
  ]}
  direction="horizontal"
  ariaLabel="Flow diagram showing the layers of an AI application"
/>

Each layer shapes behavior. The combination defines the application.

<Recognition>

You've experienced this variety. ChatGPT feels different from Claude, which feels different from Copilot, which feels different from a customer service botâ€”even when powered by similar base models. The wrapping makes the difference.

</Recognition>

**System prompts: defining persona and behavior**

The **system prompt** is invisible instructions given to the model before your message. It shapes personality, establishes rules, and provides context.

```
System prompt for a customer service bot:

You are a helpful customer service agent for TechCorp.
Be friendly, concise, and solution-oriented.
Never discuss competitor products.
If you can't help, offer to escalate to a human agent.
Always greet the customer by name if known.
```

The user never sees this. But it fundamentally changes how the model responds. Same LLM, completely different behavior.

<Question title="What can system prompts do?">

System prompts can:

- **Set persona**: Formal assistant, casual friend, domain expert
- **Define scope**: What topics to address, what to refuse
- **Establish rules**: Response format, length limits, required disclaimers
- **Inject knowledge**: Company info, product details, policies
- **Shape tone**: Enthusiastic, measured, empathetic, professional

They're "soft" controlsâ€”the model usually follows them but isn't guaranteed to. For hard guarantees, you need guardrails.

</Question>

**Guardrails: hard rules on input and output**

System prompts suggest behavior. **Guardrails** enforce it with code, not just instructions.

Input guardrails filter user messages before they reach the modelâ€”blocking prohibited content, detecting prompt injection, filtering sensitive data. Output guardrails check responses before they reach usersâ€”removing harmful content, ensuring format compliance, validating claims.

Guardrails provide the safety margin that soft prompts can't guarantee. They're how you turn "usually safe" into "reliably safe."

â†’ [Guardrails in depth](/guardrails)

**Retrieval-Augmented Generation (RAG): grounding in real information**

LLMs have vast knowledge from trainingâ€”but that knowledge is frozen at the cutoff date, may be incomplete, and can't cover your private data. What if you need answers about *your* documents, *your* database, *your* codebase?

**RAG** solves this. Before generating a response, the system retrieves relevant information and includes it in the prompt. The model reasons over retrieved content, not just training memory.

<FlowDiagram
  title="RAG Pipeline"
  steps={[
    { id: '1', label: 'User Question', sublabel: '"What\'s our refund policy?"', icon: 'ðŸ’¬' },
    { id: '2', label: 'Embed Query', sublabel: 'Convert to vector', icon: 'ðŸ”¢' },
    { id: '3', label: 'Search', sublabel: 'Find similar docs', icon: 'ðŸ”' },
    { id: '4', label: 'Retrieve', sublabel: 'Get relevant chunks', icon: 'ðŸ“„' },
    { id: '5', label: 'Generate', sublabel: 'LLM + retrieved context', icon: 'âœ¨' },
  ]}
  direction="horizontal"
  ariaLabel="Flow diagram showing RAG pipeline from question to answer"
/>

The model doesn't need to "know" the answer from training. It reads the answer from retrieved text and synthesizes a response. Suddenly the LLM can answer questions about things it never saw in training.

<Question title="How does retrieval actually work?">

RAG typically uses **semantic search** via vector databases:

1. **Index your documents**: Split into chunks, convert each to an embedding (vector), store in a vector database
2. **At query time**: Convert the user's question to an embedding
3. **Search**: Find document chunks with similar embeddings
4. **Inject**: Add retrieved chunks to the prompt as context

The embedding similarity captures meaning, not just keywords. "What's the refund policy?" matches documents about "return procedures" even without shared words.

â†’ [Vector databases in depth](/vector-databases)

</Question>

**Why RAG instead of fine-tuning?**

You could train the model on your data. But:

- **Fine-tuning is expensive**: Compute, data prep, training runs
- **Knowledge becomes stale**: Retrain when data changes
- **No transparency**: Can't see what the model "knows"

RAG offers:

- **Dynamic updates**: Change documents, answers update immediately
- **Transparency**: See exactly what was retrieved
- **Scale**: Millions of documents without retraining

**RAG reduces hallucination** by grounding responses in actual documents. Instead of inventing plausible-sounding details, the model synthesizes from retrieved evidence.

<Question title="How is RAG different from tool use?">

Both give LLMs access to external information, but differently:

**RAG**:
- Retrieval happens *before* generation
- Context injected into prompt automatically
- User query drives semantic search
- Works without model decision

**Tool use**:
- Model *decides* to call a tool during generation
- Tools can do more than retrieve: calculate, act, search web
- Explicit tool definitions required

**When to use which**:
- RAG: Knowledge bases, document Q&A, grounding responses
- Tools: Web search, calculations, actions, dynamic data

Many systems combine both: RAG for knowledge, tools for actions.

</Question>

**Tools: taking actions beyond text**

Tools let the model do things:

- **Search**: Web search, database queries, API calls
- **Calculate**: Math, code execution, data analysis
- **Act**: Send emails, create tickets, update records
- **Create**: Generate images, produce documents, write code

An application defines which tools are available. A coding assistant has different tools than a customer service bot, which has different tools than a research assistant.

The tool selection shapes what the application can do. An LLM without tools can only talk. An LLM with tools can work.

**Agentic loops: multi-step reasoning**

Simple applications are one-shot: user asks, model answers. **Agents** are iterative: the model plans, uses tools, observes results, and repeats until the task is complete.

This is what enables AI to book flights, research topics across multiple sources, or refactor code across many filesâ€”tasks requiring multiple steps and adaptive decision-making.

â†’ [How agents work](/agents)

<Metaphor title="A race car vs. a family sedan">

An LLM is an engineâ€”powerful, capable, but not a vehicle.

Wrapping it as a customer service bot is like building a family sedan: comfortable, safe, predictable. It goes where customers need, with guardrails (literally) to keep everyone safe.

Wrapping it as a research assistant is like building a race car: optimized for speed, powerful tools, fewer restrictions. It goes fast and does things sedans can't, but requires a skilled driver.

Same engine, different vehicles, different experiences. The application layerâ€”the chassis, the safety features, the controlsâ€”makes all the difference.

</Metaphor>

**The composition of modern AI products**

A sophisticated AI application might combine:

- **System prompts** establishing persona and base behavior
- **RAG** providing knowledge from company docs
- **Input guardrails** blocking inappropriate requests
- **Tool access** enabling search, calculation, and actions
- **Agentic loops** for multi-step task completion
- **Output guardrails** ensuring safe, on-brand responses
- **Memory** tracking conversation history and user preferences
- **Fallbacks** escalating to humans when confidence is low

Each component is a design decision. The combination defines the product.

<TryThis>

Compare two AI products powered by the same underlying model (or similar ones). Notice the differences: How do they introduce themselves? What topics do they refuse? What actions can they take? What's their personality? These differences aren't in the modelâ€”they're in the application layer wrapped around it.

</TryThis>

**Why this matters**

When evaluating AI products, look beyond "which model does it use":

- What knowledge does it have access to?
- What tools can it use?
- What guardrails protect users?
- How is it prompted to behave?
- What happens when it fails?

The application layer often matters more than the base model. A well-wrapped smaller model can outperform a poorly-wrapped larger one for specific tasks.

<Sources>
<Citation type="paper" title="Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks" authors="Lewis et al." source="Meta AI" url="https://arxiv.org/abs/2005.11401" year={2020} />
<Citation type="docs" title="Building AI Applications" source="LangChain" url="https://python.langchain.com/docs/get_started/introduction" />
<Citation type="article" title="AI Agents" source="Anthropic" url="https://www.anthropic.com/research" />
<Citation type="docs" title="System Prompts" source="OpenAI" url="https://platform.openai.com/docs/guides/text-generation" />
</Sources>
